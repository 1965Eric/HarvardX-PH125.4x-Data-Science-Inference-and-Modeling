---
title: "Data Science Inference and Modeling"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
  word_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The textbook for the Data Science course series is [freely available online](https://rafalab.github.io/dsbook/).

This course corresponds to the textbook chapters [Statistical Inference](https://rafalab.github.io/dsbook/inference.html) and [Statistical Models](https://rafalab.github.io/dsbook/models.html).

## Learning Objectives

* The concepts necessary to define estimates and margins of errors of populations, parameters, estimates, and standard errors in order to make predictions about data
* How to use models to aggregate data from different sources
* The very basics of Bayesian statistics and predictive modeling

## Course Overview

### Section 1: Parameters and Estimates
You will learn how to estimate population parameters.

### Section 2: The Central Limit Theorem in Practice
You will apply the central limit theorem to assess how close a sample estimate is to the population parameter of interest.

### Section 3: Confidence Intervals and p-Values
You will learn how to calculate confidence intervals and learn about the relationship between confidence intervals and p-values.

### Section 4: Statistical Models
You will learn about statistical models in the context of election forecasting.

### Section 5: Bayesian Statistics
You will learn about Bayesian statistics through looking at examples from rare disease diagnosis and baseball.

### Section 6: Election Forecasting
You will learn about election forecasting, building on what you’ve learned in the previous sections about statistical modeling and Bayesian statistics.

### Section 7: Association Tests
You will learn how to use association and chi-squared tests to perform inference for binary, categorical, and ordinal data through an example looking at research funding rates.

## Introduction to Inference

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#polls)

In this course, we will learn:

* *statistical inference*, the process of deducing characteristics of a population using data from a random sample
* the statistical concepts necessary to define *estimates* and *margins of errors*
* how to *forecast future results* and estimate the precision of our forecast
* how to calculate and interpret *confidence intervals and p-values*

**Key points**

* Information gathered from a small random sample can be used to infer characteristics of the entire population.
* Opinion polls are useful when asking everyone in the population is impossible.
* A common use for opinion polls is determining voter preferences in political elections for the purposes of forecasting election results.
* The *spread* of a poll is the estimated difference between support two candidates or options.

## Section 1 Overview

Section 1 introduces you to parameters and estimates.

After completing Section 1, you will be able to:

* Understand how to use a sampling model to perform a poll.
* Explain the terms **population**, **parameter**, and **sample** as they relate to statistical inference.
* Use a sample to estimate the population proportion from the sample average.
* Calculate the expected value and standard error of the sample average.

## Sampling Model Parameters and Estimates

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#the-sampling-model-for-polls) and [here; first part](https://rafalab.github.io/dsbook/inference.html#populations-samples-parameters-and-estimates)

**Key points**

* The task of statistical inference is to estimate an unknown population parameter using observed data from a sample.
* In a sampling model, the collection of elements in the urn is called the *population*.
* A *parameter* is a number that summarizes data for an entire population.
* A *sample* is observed data from a subset of the population.
* An *estimate* is a summary of the observed data about a parameter that we believe is informative. It is a data-driven guess of the population parameter.
* We want to predict the proportion of the blue beads in the urn, the parameter $p$ . The proportion of red beads in the urn is $1 - p$ and the *spread* is $2p - 1$.
* The sample proportion is a random variable. Sampling gives random results drawn from the population distribution.

*Code: Function for taking a random draw from a specific urn*

The **dslabs** package includes a function for taking a random draw of size $n$ from the urn:

```{r}
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(dslabs)) install.packages("dslabs")

library(tidyverse)
library(dslabs)
take_poll(25)    # draw 25 beads
```

## The Sample Average

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#the-sample-average) and [here](https://rafalab.github.io/dsbook/inference.html#parameters)

**Key points**

* Many common data science tasks can be framed as estimating a parameter from a sample.
* We illustrate statistical inference by walking through the process to estimate $p$. From the estimate of $p$, we can easily calculate an estimate of the spread, $2p - 1$.
* Consider the random variable $X$ that is 1 if a blue bead is chosen and 0 if a red bead is chosen. The proportion of blue beads in $N$ draws is the average of the draws $X_1,...,X_N$.
* $\overline{X}$ is the *sample average*. In statistics, a bar on top of a symbol denotes the average. $\overline{X}$ is a random variable because it is the average of random draws - each time we take a sample, $\overline{X}$ is different.

$\overline{X} = \frac{X_1+X_2+...+X_N}{N}$

* The number of blue beads drawn in N draws, $N \overline{X}$, is $N$ times the proportion of values in the urn. However, we do not know the true proportion: we are trying to estimate this parameter $p$.

## Polling versus Forecasting

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#polling-versus-forecasting)

**Key points**

* A poll taken in advance of an election estimates $p$ for that moment, not for election day.
* In order to predict election results, forecasters try to use early estimates of $p$ to predict $p$ on election day. We discuss some approaches in later sections.

## Properties of Our Estimate

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#properties-of-our-estimate-expected-value-and-standard-error)

**Key points**

* When interpreting values of $\overline{X}$, it is important to remember that $\overline{X}$ is a random variable with an expected value and standard error that represents the sample proportion of positive events.
* The expected value of $\overline{X}$ is the parameter of interest $p$. This follows from the fact that $\overline{X}$ is the sum of independent draws of a random variable times a constant $1/N$.

$E(\overline{X}) = p$

* As the number of draws $N$ increases, the standard error of our estimate $\overline{X}$ decreases. The standard error of the average of $\overline{X}$ over $N$ draws is:

$SE(\overline{X}) = \sqrt{p(1-p)/N}$

* In theory, we can get more accurate estimates of $p$ by increasing $N$. In practice, there are limits on the size of $N$ due to costs, as well as other factors we discuss later.
* We can also use other random variable equations to determine the expected value of the sum of draws $E(S)$ and standard error of the sum of draws $SE(S)$.

$E(S) = Np$ 

$SE(S) = \sqrt{Np(1-p)}$ 

## Assessment - Parameters and Estimates

1. Suppose you poll a population in which a proportion $p$ of voters are Democrats and $1-p$ are Republicans. 

Your sample size is $N = 25$. Consider the random variable $S$, which is the **total** number of Democrats in your sample.

What is the expected value of this random variable $S$?

- [ ] A.  $E(S) = 25(1−p)$
- [X] B.  $E(S) = 25p$
- [ ] C.  $E(S) = \sqrt{25p(1−p)}$
- [ ] D.  $E(S) = p$

2. Again, consider the random variable $S$, which is the **total** number of Democrats in your sample of 25 voters. 

The variable $p$ describes the proportion of Democrats in the sample, whereas $1-p$ describes the proportion of Republicans.

What is the standard error of $S$?

- [ ] A.  $SE(S) = 25p(1−p)$
- [ ] B.  $SE(S) = \sqrt{25p}$
- [ ] C.  $SE(S) = 25(1−p)$
- [X] D.  $SE(S) = \sqrt{25p(1−p)}$

3. Consider the random variable $S/N$, which is equivalent to the sample average that we have been denoting as $\overline{X}$. 

The variable $N$ represents the sample size and $p$ is the proportion of Democrats in the population.

What is the expected value of $\overline{X}$?

- [X] A.  $E(\overline{X}) = p$
- [ ] B.  $E(\overline{X}) = Np$
- [ ] C.  $E(\overline{X}) = N(1−p)$
- [ ] D.  $E(\overline{X}) = 1−p$

4. What is the standard error of the sample average, $\overline{X}$?

The variable $N$ represents the sample size and $p$ is the proportion of Democrats in the population.

- [ ] A.  $SE(\overline{X}) = \sqrt{Np(1−p)}$
- [X] B.  $SE(\overline{X}) = \sqrt{p(1−p)/N}$
- [ ] C.  $SE(\overline{X}) = \sqrt{p(1−p)}$
- [ ] D.  $SE(\overline{X}) = \sqrt N$

5. Write a line of code that calculates the standard error ```se``` of a sample average when you poll 25 people in the population. 

Generate a sequence of 100 proportions of Democrats ```p``` that vary from 0 (no Democrats) to 1 (all Democrats).

Plot ```se``` versus ```p``` for the 100 different proportions.

```{r}
# `N` represents the number of people polled
N <- 25

# Create a variable `p` that contains 100 proportions ranging from 0 to 1 using the `seq` function
p <- seq(0, 1, length.out = 100)

# Create a variable `se` that contains the standard error of each sample average
se <- sqrt(p * (1 - p)/N)

# Plot `p` on the x-axis and `se` on the y-axis
plot(p,se)
```

6. Using the same code as in the previous exercise, create a for-loop that generates three plots of ```p``` versus ```se``` when the sample sizes equal $N = 25$, $N = 100$, and $N = 1000$.

```{r}
# The vector `p` contains 100 proportions of Democrats ranging from 0 to 1 using the `seq` function
p <- seq(0, 1, length = 100)

# The vector `sample_sizes` contains the three sample sizes
sample_sizes <- c(25, 100, 1000)

# Write a for-loop that calculates the standard error `se` for every value of `p` for each of the three samples sizes `N` in the vector `sample_sizes`. Plot the three graphs, using the `ylim` argument to standardize the y-axis across all three plots.
for (N in sample_sizes)
{
se <- sqrt(p * (1 - p)/N)
plot(p,se,ylim = c(0,0.1))
}
```

7. Our estimate for the difference in proportions of Democrats and Republicans is $d = \overline{X} − (1 − \overline{X})$.

Which derivation correctly uses the rules we learned about sums of random variables and scaled random variables to derive the expected value of $d$

- [ ] A.  $E \left [\overline{X} − (1 − \overline{X}) \right] = E \left [2 \overline{X} − 1 \right] = 2E \left [\overline{X} \right] - 1 = N(2p − 1) = Np − N(1 − p)$
- [ ] B.  $E \left [\overline{X} − (1 − \overline{X}) \right] = E \left [\overline{X} − 1 \right] = E \left [\overline{X} \right] − 1 = p − 1$
- [ ] C.  $E \left [\overline{X} − (1 − \overline{X}) \right] = E \left [2 \overline{X} − 1 \right] = 2E \left [\overline{X} \right] - 1 = 2 \sqrt{p(1 − p)} − 1 = p − (1 − p)$
- [X] D.  $E \left [\overline{X} − (1 − \overline{X}) \right] = E \left [2 \overline{X} − 1 \right] = 2E \left [\overline{X} \right] - 1 = 2p − 1 = p − (1 − p)$

8. Our estimate for the difference in proportions of Democrats and Republicans is $d = \overline{X} − (1 − \overline{X})$.

Which derivation correctly uses the rules we learned about sums of random variables and scaled random variables to derive the standard error of $d$?

- [ ] A.  $SE \left [\overline{X} − (1 − \overline{X}) \right] = SE \left [2 \overline{X} − 1 \right] = 2SE \left [\overline{X} \right] = 2 \sqrt{p/N}$
- [ ] B.  $SE \left [\overline{X} − (1 − \overline{X}) \right] = SE \left [2 \overline{X} − 1 \right] = 2SE \left [\overline{X} - 1 \right] = 2 \sqrt{p(1 − p)/N} − 1$
- [X] C.  $SE \left [\overline{X} − (1 − \overline{X}) \right] = SE \left [2 \overline{X} − 1 \right] = 2SE \left [\overline{X} \right] = 2 \sqrt{p(1 − p)/N}$
- [ ] D.  $SE \left [\overline{X} − (1 − \overline{X}) \right] = SE \left [\overline{X} − 1 \right] = SE\left [\overline{X} \right] = \sqrt{p(1 − p)/N}$

9. Say the actual proportion of Democratic voters is $p = 0.45$. 

In this case, the Republican party is winning by a relatively large margin of $d = -0.1$, or a 10% margin of victory. What is the standard error of the spread $2 \overline{X} − 1$ in this case?

```{r}
# `N` represents the number of people polled
N <- 25

# `p` represents the proportion of Democratic voters
p <- 0.45

# Calculate the standard error of the spread. Print this value to the console.
2*sqrt((p*(1-p)/N))
```

10. So far we have said that the difference between the proportion of Democratic voters and Republican voters is about 10% and that the standard error of this spread is about 0.2 when $N = 25$. 

Select the statement that explains why this sample size is sufficient or not.

- [ ] A. This sample size is sufficient because the expected value of our estimate $2 \overline{X} − 1$ is $d$ so our prediction will be right on.
- [X] B. This sample size is too small because the standard error is larger than the spread.
- [ ] C. This sample size is sufficient because the standard error of about 0.2 is much smaller than the spread of 10%.
- [ ] D. Without knowing ```p```, we have no way of knowing that increasing our sample size would actually improve our standard error.

## Section 2 Overview

In Section 2, you will look at the Central Limit Theorem in practice.

After completing Section 2, you will be able to:

* Use the Central Limit Theorem to calculate the probability that a sample estimate $\overline{X}$ is close to the population proportion $p$.
* Run a Monte Carlo simulation to corroborate theoretical results built using probability theory.
* Estimate the spread based on estimates of $\overline{X}$ and $\hat{SE}(\overline{X})$.
* Understand why bias can mean that larger sample sizes aren’t necessarily better.

## The Central Limit Theorem in Practice

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#clt)

**Key points**

* Because $\overline{X}$ is the sum of random draws divided by a constant, the distribution of $\overline{X}$ is approximately normal.
* We can convert $\overline{X}$ to a standard normal random variable $Z$: 

$Z = \frac{\overline{X} - E(\overline{X})}{SE(\overline{X})}$

* The probability that $\overline{X}$ is within .01 of the actual value of $p$ is:

$Pr(Z \le .01/\sqrt {p(1 - p)/N}) - Pr(Z \le -.01/\sqrt {p(1 - p)/N})$

* The Central Limit Theorem (CLT) still works if $\overline{X}$ is used in place of $p$. This is called a *plug-in estimate*. Hats over values denote estimates. Therefore:

$\hat{SE}(\overline{X}) = \sqrt{\overline{X}(1 - \overline{X})/N}$

Using the CLT, the probability that $\overline{X}$ is within .01 of the actual value of $p$ is:

$Pr(Z \le .01/\sqrt {\overline{X}(1 - \overline{X})/N}) - Pr(Z \le -.01/\sqrt {\overline{X}(1 - \overline{X})/N})$

*Code: Computing the probability of $\overline{X}$ being within .01 of $p$*

```{r}
X_hat <- 0.48
se <- sqrt(X_hat*(1-X_hat)/25)
pnorm(0.01/se) - pnorm(-0.01/se)
```

## Margin of Error

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#clt)

**Key points**

* The *margin of error* is defined as 2 times the standard error of the estimate $\overline{X}$.
* There is about a 95% chance that $\overline{X}$ will be within two standard errors of the actual parameter $p$.

## A Monte Carlo Simulation for the CLT

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#a-monte-carlo-simulation)

**Key points**

* We can run Monte Carlo simulations to compare with theoretical results assuming a value of $p$.
* In practice, $p$ is unknown. We can corroborate theoretical results by running Monte Carlo simulations with one or several values of $p$.
* One practical choice for $p$ when modeling is $\overline{X}$, the observed value of $\hat{X}$ in a sample.

*Code: Monte Carlo simulation using a set value of p*

```{r}
p <- 0.45    # unknown p to estimate
N <- 1000

# simulate one poll of size N and determine x_hat
x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)

# simulate B polls of size N and determine average x_hat
B <- 10000    # number of replicates
N <- 1000    # sample size per replicate
x_hat <- replicate(B, {
    x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    mean(x)
})
```

*Code: Histogram and QQ-plot of Monte Carlo results*

```{r}
if(!require(gridExtra)) install.packages("gridExtra")

library(gridExtra)
p1 <- data.frame(x_hat = x_hat) %>%
    ggplot(aes(x_hat)) +
    geom_histogram(binwidth = 0.005, color = "black")
p2 <- data.frame(x_hat = x_hat) %>%
    ggplot(aes(sample = x_hat)) +
    stat_qq(dparams = list(mean = mean(x_hat), sd = sd(x_hat))) +
    geom_abline() +
    ylab("X_hat") +
    xlab("Theoretical normal")
grid.arrange(p1, p2, nrow=1)
```

## The Spread

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#the-spread)

**Key points**

* The spread between two outcomes with probabilities $p$ and $1 - p$ is $2p - 1$.
* The expected value of the spread is $2 \overline{X} - 1$.
* The standard error of the spread is $2 \hat{SE}(\overline{X})$.
* The margin of error of the spread is 2 times the margin of error of $\overline{X}$.

## Bias: Why Not Run a Very Large Poll?

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#bias-why-not-run-a-very-large-poll)

**Key points**

* An extremely large poll would theoretically be able to predict election results almost perfectly.
* These sample sizes are not practical. In addition to cost concerns, polling doesn't reach everyone in the population (eventual voters) with equal probability, and it also may include data from outside our population (people who will not end up voting).
* These systematic errors in polling are called *bias*. We will learn more about bias in the future.

*Code: Plotting margin of error in an extremely large poll over a range of values of p*

```{r}
N <- 100000
p <- seq(0.35, 0.65, length = 100)
SE <- sapply(p, function(x) 2*sqrt(x*(1-x)/N))
data.frame(p = p, SE = SE) %>%
    ggplot(aes(p, SE)) +
    geom_line()
```

## Assessment - Introduction to Inference

1. Write function called ```take_sample``` that takes the proportion of Democrats $p$ and the sample size $N$ as arguments and returns the sample average of Democrats (1) and Republicans (0).

Calculate the sample average if the proportion of Democrats equals 0.45 and the sample size is 100.

```{r}
# Write a function called `take_sample` that takes `p` and `N` as arguements and returns the average value of a randomly sampled population.
take_sample <- function(p, N) {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  return(mean(x))
}

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Call the `take_sample` function to determine the sample average of `N` randomly selected people from a population containing a proportion of Democrats equal to `p`. Print this value to the console.
take_sample(p, N)
```

2. Assume the proportion of Democrats in the population $p$ equals 0.45 and that your sample size $N$ is 100 polled voters. 

The ```take_sample``` function you defined previously generates our estimate, $\overline{X}$.

Replicate the random sampling 10,000 times and calculate $p − \overline{X}$ for each random sample. Save these differences as a vector called ```errors```. Find the average of ```errors``` and plot a histogram of the distribution.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Create an objected called `errors` that replicates subtracting the result of the `take_sample` function from `p` for `B` replications
errors <- replicate(B, p - take_sample(p, N))

# Calculate the mean of the errors. Print this value to the console.
mean(errors)
hist(errors)
```

3. In the last exercise, you made a vector of differences between the actual value for $p$ and an estimate, $\overline{X}$. 

We called these differences between the actual and estimated values ```errors```.

The ```errors``` object has already been loaded for you. Use the ```hist``` function to plot a histogram of the values contained in the vector ```errors```. Which statement best describes the distribution of the errors?

- [ ] A. The errors are all about 0.05.
- [ ] B. The errors are all about -0.05.
- [X] C. The errors are symmetrically distributed around 0.
- [ ] D. The errors range from -1 to 1.

4. The error $p - \overline{X}$ is a random variable. 

In practice, the error is not observed because we do not know the actual proportion of Democratic voters, $p$. However, we can describe the size of the error by constructing a simulation.

What is the average size of the error if we define the size by taking the absolute value $|p - \overline{X}|$?

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the mean of the absolute value of each simulated error. Print this value to the console.
mean(abs(errors))
```

5. The standard error is related to the typical **size** of the error we make when predicting. 

We say **size** because, as we just saw, the errors are centered around 0. In that sense, the typical error is 0. For mathematical reasons related to the central limit theorem, we actually use the standard deviation of ```errors``` rather than the average of the absolute values.

As we have discussed, the standard error is the square root of the average squared distance $(\overline{X} - p)^2$. The standard deviation is defined as the square root of the distance squared.

Calculate the standard deviation of the spread.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# We generated `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Calculate the standard deviation of `errors`
sqrt(mean(errors^2))
```

6. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of $\overline{X}$.

Estimate the standard error given an expected value of 0.45 and a sample size of 100.

```{r}
# Define `p` as the expected value equal to 0.45
p <- 0.45

# Define `N` as the sample size
N <- 100

# Calculate the standard error
sqrt(p*(1-p)/N)
```

7. In practice, we don't know $p$, so we construct an estimate of the theoretical prediction based by plugging in $\overline{X}$ for $p$. Calculate the standard error of the estimate: $\hat{SE}(\overline{X})$

```{r}
# Define `p` as a proportion of Democratic voters to simulate
p <- 0.45

# Define `N` as the sample size
N <- 100

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `X` as a random sample of `N` voters with a probability of picking a Democrat ('1') equal to `p`
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))

# Define `X_bar` as the average sampled proportion
X_bar <- mean(X)

# Calculate the standard error of the estimate. Print the result to the console.
se <- sqrt((X_bar*(1-X_bar)/N))
se
```

8. The standard error estimates obtained from the Monte Carlo simulation, the theoretical prediction, and the estimate of the theoretical prediction are all very close, which tells us that the theory is working. 

This gives us a practical approach to knowing the typical error we will make if we predict $p$ with $\hat{X}$. The theoretical result gives us an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for $p = 0.5$.

Create a plot of the largest standard error for $N$ ranging from 100 to 5,000. 

```{r}
N <- seq(100, 5000, len = 100)
p <- 0.5
se <- sqrt(p*(1-p)/N)
plot(se, N)
```

Based on this plot, how large does the sample size have to be to have a standard error of about 1%?

- [ ] A. 100
- [ ] B. 500
- [X] C. 2,500
- [ ] D. 4,000

9. For $N = 100$, the central limit theorem tells us that the distribution of $\hat{X}$ is…

- [ ] A. practically equal to $p$.
- [X] B. approximately normal with expected value $p$ and standard error $\sqrt{p(1 − p)/N}$.
- [ ] C. approximately normal with expected value $\overline{X}$ and standard error $\sqrt{\overline{X}(1 − \overline{X}/N}$.
- [ ] D. not a random variable.

10. We calculated a vector ```errors``` that contained, for each simulated sample, the difference between the actual value ```p``` and our estimate $\hat{X}$.

The errors $\overline{X} − p$ are:

- [ ] A. practically equal to 0.
- [X] B. approximately normal with expected value 0 and standard error $\sqrt{p(1 − p)/N}$.
- [ ] C. approximately normal with expected value p and standard error $\sqrt{p(1 − p)/N}$.
- [ ] D. not a random variable.

11. Make a qq-plot of the ```errors``` you generated previously to see if they follow a normal distribution.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Generate `errors` by subtracting the estimate from the actual proportion of Democratic voters
errors <- replicate(B, p - take_sample(p, N))

# Generate a qq-plot of `errors` with a qq-line showing a normal distribution
qqnorm(errors)
qqline(errors)
```

12. If $p = 0.45$ and $N = 100$, use the central limit theorem to estimate the probability that $\overline{X} > 0.5$.

```{r}
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45

# Define `N` as the number of people polled
N <- 100

# Calculate the probability that the estimated proportion of Democrats in the population is greater than 0.5. Print this value to the console.

1-pnorm(0.5, p, sqrt(p*(1-p)/N))
```

13. Assume you are in a practical situation and you don't know $p$. 

Take a sample of size $N = 100$ and obtain a sample average of $\overline{X} = 0.51$.

What is the CLT approximation for the probability that your error size is equal or larger than 0.01?

```{r}
# Define `N` as the number of people polled
N <-100

# Define `X_hat` as the sample average
X_hat <- 0.51

# Define `se_hat` as the standard error of the sample average
se_hat <- sqrt(X_hat*(1-X_hat)/N)

# Calculate the probability that the error is 0.01 or larger
1-pnorm(0.01,0,se_hat) + pnorm(-0.01,0,se_hat)
```

## Section 3 Overview

In Section 3, you will look at confidence intervals and p-values.

After completing Section 3, you will be able to:

* Calculate confidence intervals of difference sizes around an estimate.
* Understand that a confidence interval is a random interval with the given probability of falling on top of the parameter.
* Explain the concept of “power” as it relates to inference.
* Understand the relationship between p-values and confidence intervals and explain why reporting confidence intervals is often preferable.

## Confidence Intervals

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#confidence-intervals)

**Key points**

* We can use statistical theory to compute the probability that a given interval contains the true parameter $p$.
* 95% confidence intervals are intervals constructed to have a 95% chance of including $p$. The margin of error is approximately a 95% confidence interval.
* The start and end of these confidence intervals are random variables.
* To calculate any size confidence interval, we need to calculate the value $z$ for which $Pr(-z \le Z \le z)$ equals the desired confidence. For example, a 99% confidence interval requires calculating $z$ for $Pr(-z \le Z \le z) = 0.99$.
* For a confidence interval of size $q$, we solve for $z = 1 - \frac{1 - q}{2}$.
* To determine a 95% confidence interval, use ```z <- qnorm(0.975)```. This value is slightly smaller than 2 times the standard error.

*Code: geom_smooth confidence interval example*

The shaded area around the curve is related to the concept of confidence intervals.

```{r}
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
    ggplot(aes(year, temperature)) +
    geom_point() +
    geom_smooth() +
    ggtitle("Average Yearly Temperatures in New Haven")
```

*Code: Monte Carlo simulation of confidence intervals*

Note that to compute the exact 95% confidence interval, we would use ```qnorm(.975)*SE_hat``` instead of ```2*SE_hat```.

```{r}
p <- 0.45
N <- 1000
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))    # generate N observations
X_hat <- mean(X)    # calculate X_hat
SE_hat <- sqrt(X_hat*(1-X_hat)/N)    # calculate SE_hat, SE of the mean of N observations
c(X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # build interval of 2*SE above and below mean
```

*Code: Solving for $z$ with ```qnorm```*

```{r}
z <- qnorm(0.995)    # calculate z to solve for 99% confidence interval
pnorm(qnorm(0.995))    # demonstrating that qnorm gives the z value for a given probability
pnorm(qnorm(1-0.995))    # demonstrating symmetry of 1-qnorm
pnorm(z) - pnorm(-z)    # demonstrating that this z value gives correct probability for interval
```

## A Monte Carlo Simulation for Confidence Intervals

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#a-monte-carlo-simulation-1)

**Key points**

* We can run a Monte Carlo simulation to confirm that a 95% confidence interval contains the true value of $p$ 95% of the time.
* A plot of confidence intervals from this simulation demonstrates that most intervals include $p$, but roughly 5% of intervals miss the true value of $p$.

*Code: Monte Carlo simulation*

Note that to compute the exact 95% confidence interval, we would use ```qnorm(.975)*SE_hat``` instead of ```2*SE_hat```.

```{r}
B <- 10000
inside <- replicate(B, {
    X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
    X_hat <- mean(X)
    SE_hat <- sqrt(X_hat*(1-X_hat)/N)
    between(p, X_hat - 2*SE_hat, X_hat + 2*SE_hat)    # TRUE if p in confidence interval
})
mean(inside)
```

## The Correct Language

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#the-correct-language)

**Key points**

* The 95% confidence intervals are random, but $p$ is not random.
* 95% refers to the probability that the random interval falls on top of $p$.
* It is technically incorrect to state that $p$ has a 95% chance of being in between two values because that implies $p$ is random.

## Power

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#power)

**Key points**

* If we are trying to predict the result of an election, then a confidence interval that includes a spread of 0 (a tie) is not helpful.
* A confidence interval that includes a spread of 0 does not imply a close election, it means the sample size is too small.
* Power is the probability of detecting an effect when there is a true effect to find. Power increases as sample size increases, because larger sample size means smaller standard error.

*Code: Confidence interval for the spread with sample size of 25*

Note that to compute the exact 95% confidence interval, we would use ```c(-qnorm(.975), qnorm(.975))``` instead of 1.96.

```{r}
N <- 25
X_hat <- 0.48
(2*X_hat - 1) + c(-2, 2)*2*sqrt(X_hat*(1-X_hat)/N)
```

## p-Values

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#p-values)

**Key points**

* The null hypothesis is the hypothesis that there is no effect. In this case, the null hypothesis is that the spread is 0, or $p = 0.5$.
* The p-value is the probability of detecting an effect of a certain size or larger when the null hypothesis is true.
* We can convert the probability of seeing an observed value under the null hypothesis into a standard normal random variable. We compute the value of $z$ that corresponds to the observed result, and then use that $z$ to compute the p-value.
* If a 95% confidence interval does not include our observed value, then the p-value must be smaller than 0.05.
* It is preferable to report confidence intervals instead of p-values, as confidence intervals give information about the size of the estimate and p-values do not.

*Code: Computing a p-value for observed spread of 0.02*

```{r}
N <- 100    # sample size
z <- sqrt(N) * 0.02/0.5    # spread of 0.02
1 - (pnorm(z) - pnorm(-z))
```

## Another Explanation of p-Values

The p-value is the probability of observing a value as extreme or more extreme than the result given that the null hypothesis is true. 

In the context of the normal distribution, this refers to the probability of observing a Z-score whose absolute value is as high or higher than the Z-score of interest.

Suppose we want to find the p-value of an observation 2 standard deviations larger than the mean. This means we are looking for anything with $|z| \ge 2$.

Graphically, the p-value gives the probability of an observation that's at least as far away from the mean or further. This plot shows a standard normal distribution (centered at $z = 0$) with a standard deviation of 1). The shaded tails are the region of the graph that are 2 standard deviations or more away from the mean.

![Standard normal distribution (centered at  z=0  with a standard deviation of 1](images/normal_distribution.jpg)

The p-value is the proportion of area under a normal curve that has z-scores as extreme or more extreme than the given value - the tails on this plot of a normal distribution are shaded to show the region corresponding to the p-value.

The right tail can be found with ```1-pnorm(2)```. We want to have both tails, though, because we want to find the probability of any observation as far away from the mean or farther, in either direction. (This is what's meant by a two-tailed p-value.) Because the distribution is symmetrical, the right and left tails are the same size and we know that our desired value is just ```2*(1-pnorm(2))```.

Recall that, by default, ```pnorm()``` gives the CDF for a normal distribution with a mean of $\mu = 0$ and standard deviation of $\sigma = 1$. To find p-values for a given z-score z in a normal distribution with mean ```mu``` and standard deviation ```sigma```, use ```2*(1-pnorm(z, mu, sigma))``` instead.

## Assessment - Confidence Intervals and p-Values

1. For the following exercises, we will use actual poll data from the 2016 election.

The exercises will contain pre-loaded data from the ```dslabs``` package.

```{r, eval=FALSE, echo=TRUE}
library(dslabs)
data("polls_us_election_2016")
```

We will use all the national polls that ended within a few weeks before the election.

Assume there are only two candidates and construct a 95% confidence interval for the election night proportion $p$.

```{r}
# Load the data
data(polls_us_election_2016)

# Generate an object `polls` that contains data filtered for polls that ended on or after October 31, 2016 in the United States
polls <- filter(polls_us_election_2016, enddate >= "2016-10-31" & state == "U.S.")

# How many rows does `polls` contain? Print this value to the console.
nrow(polls)

# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this value to the console.
N <- head(polls$samplesize,1)
N

# For the first poll in `polls`, assign the estimated percentage of Clinton voters to a variable called `X_hat`. Print this value to the console.
X_hat <- (head(polls$rawpoll_clinton,1)/100)
X_hat

# Calculate the standard error of `X_hat` and save it to a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(X_hat*(1-X_hat)/N)
se_hat

# Use `qnorm` to calculate the 95% confidence interval for the proportion of Clinton voters. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(X_hat - qnorm(0.975)*se_hat, X_hat + qnorm(0.975)*se_hat)
ci
```

2. Create a new object called ```pollster_results``` that contains the pollster's name, the end date of the poll, the proportion of voters who declared a vote for Clinton, the standard error of this estimate, and the lower and upper bounds of the confidence interval for the estimate.

```{r}
# The `polls` object that filtered all the data by date and nation has already been loaded. Examine it using the `head` function.
head(polls)

# Create a new object called `pollster_results` that contains columns for pollster name, end date, X_hat, se_hat, lower confidence interval, and upper confidence interval for each poll.
polls <- mutate(polls, X_hat = polls$rawpoll_clinton/100, se_hat = sqrt(X_hat*(1-X_hat)/polls$samplesize), lower = X_hat - qnorm(0.975)*se_hat, upper = X_hat + qnorm(0.975)*se_hat)
pollster_results <- select(polls, pollster, enddate, X_hat, se_hat, lower, upper)
pollster_results
```

3. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column called ```hit``` to ```pollster_results``` that states if the confidence interval included the true proportion $p=0.482$ or not. What proportion of confidence intervals included $p$?

```{r}
# The `pollster_results` object has already been loaded. Examine it using the `head` function.
head(pollster_results)

# Add a logical variable called `hit` that indicates whether the actual value exists within the confidence interval of each poll. Summarize the average `hit` result to determine the proportion of polls with confidence intervals include the actual value. Save the result as an object called `avg_hit`.
avg_hit <- pollster_results %>% mutate(hit=(lower<0.482 & upper>0.482)) %>% summarize(mean(hit))
avg_hit
```

4. If these confidence intervals are constructed correctly, and the theory holds up, what proportion of confidence intervals should include $p$?

- [ ] A. 0.05
- [ ] B. 0.31
- [ ] C. 0.50
- [X] D. 0.95

5. A much smaller proportion of the polls than expected produce confidence intervals containing $p$. 

Notice that most polls that fail to include $p$ are underestimating. The rationale for this is that undecided voters historically divide evenly between the two main candidates on election day.

In this case, it is more informative to estimate the spread or the difference between the proportion of two candidates $d$, or $0.482 − 0.461 = 0.021$ for this election.

Assume that there are only two parties and that $d = 2p − 1$. Construct a 95% confidence interval for difference in proportions on election night.

```{r}
# Add a statement to this line of code that will add a new column named `d_hat` to `polls`. The new column should contain the difference in the proportion of voters.
polls <- polls_us_election_2016 %>% filter(enddate >= "2016-10-31" & state == "U.S.")  %>%
  mutate(d_hat = rawpoll_clinton/100 - rawpoll_trump/100)

# Assign the sample size of the first poll in `polls` to a variable called `N`. Print this value to the console.
N <- polls$samplesize[1]
N

# Assign the difference `d_hat` of the first poll in `polls` to a variable called `d_hat`. Print this value to the console.
d_hat <- polls$d_hat[1]
d_hat

# Assign proportion of votes for Clinton to the variable `X_hat`.
X_hat <- (d_hat+1)/2
X_hat

# Calculate the standard error of the spread and save it to a variable called `se_hat`. Print this value to the console.
se_hat <- 2*sqrt(X_hat*(1-X_hat)/N)
se_hat


# Use `qnorm` to calculate the 95% confidence interval for the difference in the proportions of voters. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(d_hat - qnorm(0.975)*se_hat, d_hat + qnorm(0.975)*se_hat)
ci
```

6. Create a new object called ```pollster_results``` that contains the pollster's name, the end date of the poll, the difference in the proportion of voters who declared a vote either, and the lower and upper bounds of the confidence interval for the estimate.

```{r}
# The subset `polls` data with 'd_hat' already calculated has been loaded. Examine it using the `head` function.
head(polls)

# Create a new object called `pollster_results` that contains columns for pollster name, end date, d_hat, lower confidence interval of d_hat, and upper confidence interval of d_hat for each poll.
d_hat = polls$rawpoll_clinton/100 - polls$rawpoll_trump/100
X_hat = (d_hat + 1) / 2
polls <- mutate(polls, X_hat, se_hat = 2 * sqrt(X_hat * (1 - X_hat) / samplesize), lower = d_hat - qnorm(0.975)*se_hat, upper = d_hat + qnorm(0.975)*se_hat)
pollster_results <- select(polls, pollster, enddate, d_hat, lower, upper)
pollster_results
```

7. What proportion of confidence intervals for the difference between the proportion of voters included $d$, the actual difference in election day?

```{r}
# The `pollster_results` object has already been loaded. Examine it using the `head` function.
head(pollster_results)

# Add a logical variable called `hit` that indicates whether the actual value (0.021) exists within the confidence interval of each poll. Summarize the average `hit` result to determine the proportion of polls with confidence intervals include the actual value. Save the result as an object called `avg_hit`.
avg_hit <- pollster_results %>% mutate(hit=(lower<0.021 & upper>0.021)) %>% summarize(mean(hit))
avg_hit
```

8. Although the proportion of confidence intervals that include the actual difference between the proportion of voters increases substantially, it is still lower that 0.95. 

In the next chapter, we learn the reason for this. To motivate our next exercises, calculate the difference between each poll's estimate $\overline{d}$ and the actual $d = 0.021$. Stratify this difference, or error, by pollster in a plot.

```{r}
# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)

# Add variable called `error` to the object `polls` that contains the difference between d_hat and the actual difference on election day. Then make a plot of the error stratified by pollster.
error <- polls$d_hat - 0.021
polls <- mutate(polls, error)
polls %>% ggplot(aes(x = pollster, y = error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

9. Remake the plot you made for the previous exercise, but only for pollsters that took five or more polls.

You can use dplyr tools ```group_by``` and ```n``` to group data by a variable of interest and then count the number of observations in the groups. The function ```filter``` filters data piped into it by your specified condition.

For example:

```{r, eval=FALSE, echo=TRUE}
data %>% group_by(variable_for_grouping) 
    %>% filter(n() >= 5)
```

```{r}
# The `polls` object has already been loaded. Examine it using the `head` function.
head(polls)

# Add variable called `error` to the object `polls` that contains the difference between d_hat and the actual difference on election day. Then make a plot of the error stratified by pollster, but only for pollsters who took 5 or more polls.
error <- polls$d_hat - 0.021
polls <- mutate(polls, error)
polls %>% group_by(pollster) %>% filter(n() >= 5) 
polls %>% ggplot(aes(x = pollster, y = error)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Section 4 Overview

In Section 4, you will look at statistical models in the context of election polling and forecasting.

After completing Section 4, you will be able to:

* Understand how aggregating data from different sources, as poll aggregators do for poll data, can improve the precision of a prediction.
* Understand how to fit a multilevel model to the data to forecast, for example, election results.
* Explain why a simple aggregation of data is insufficient to combine results because of factors such as pollster bias.
* Use a data-driven model to account for additional types of sampling variability such as pollster-to-pollster variability.

## Poll Aggregators

The textbook for this section is available [here](https://rafalab.github.io/dsbook/models.html) and [here](https://rafalab.github.io/dsbook/models.html#poll-aggregators)

**Key points**

* Poll aggregators combine the results of many polls to simulate polls with a large sample size and therefore generate more precise estimates than individual polls.
* Polls can be simulated with a Monte Carlo simulation and used to construct an estimate of the spread and confidence intervals.
* The actual data science exercise of forecasting elections involves more complex statistical modeling, but these underlying ideas still apply.

*Code: Simulating polls*

Note that to compute the exact 95% confidence interval, we would use ```qnorm(.975)*SE_hat``` instead of ```2*SE_hat```.

```{r}
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d+1)/2

# calculate confidence intervals of the spread
confidence_intervals <- sapply(Ns, function(N){
    X <- sample(c(0,1), size=N, replace=TRUE, prob = c(1-p, p))
    X_hat <- mean(X)
    SE_hat <- sqrt(X_hat*(1-X_hat)/N)
    2*c(X_hat, X_hat - 2*SE_hat, X_hat + 2*SE_hat) - 1
})

# generate a data frame storing results
polls <- data.frame(poll = 1:ncol(confidence_intervals),
                    t(confidence_intervals), sample_size = Ns)
names(polls) <- c("poll", "estimate", "low", "high", "sample_size")
polls
```

*Code: Calculating the spread of combined polls*

Note that to compute the exact 95% confidence interval, we would use ```qnorm(.975)``` instead of ```1.96```.

```{r}
d_hat <- polls %>%
    summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %>%
    .$avg

p_hat <- (1+d_hat)/2
moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size))   
round(d_hat*100,1)
round(moe*100, 1)
```

## Pollsters and Multilevel Models

The textbook for this section is available [here](https://rafalab.github.io/dsbook/models.html#poll-aggregators)

**Key points**

* Different poll aggregators generate different models of election results from the same poll data. This is because they use different statistical models.
* We will use actual polling data about the popular vote from the 2016 US presidential election to learn the principles of statistical modeling.

## Poll Data and Pollster Bias

The textbook for this section is available [here](https://rafalab.github.io/dsbook/models.html#poll-data) and [here](https://rafalab.github.io/dsbook/models.html#pollster-bias)

**Key points**

* We analyze real 2016 US polling data organized by FiveThirtyEight. We start by using reliable national polls taken within the week before the election to generate an urn model.
* Consider $p$ the proportion voting for Clinton and $1 - p$ the proportion voting for Trump. We are interested in the spread $d = 2p - 1$.
* Poll results are a random normal variable with expected value of the spread $d$ and standard error $2 \sqrt{p(1 - p)/N}$.
* Our initial estimate of the spread did not include the actual spread. Part of the reason is that different pollsters have different numbers of polls in our dataset, and each pollster has a bias.
* *Pollster bias* reflects the fact that repeated polls by a given pollster have an expected value different from the actual spread and different from other pollsters. Each pollster has a different bias.
* The urn model does not account for pollster bias. We will develop a more flexible data-driven model that can account for effects like bias.

*Code: Generating simulated poll data*

```{r}
names(polls_us_election_2016)

# keep only national polls from week before election with a grade considered reliable
polls <- polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-10-31" &
               (grade %in% c("A+", "A", "A-", "B+") | is.na(grade)))

# add spread estimate
polls <- polls %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# compute estimated spread for combined polls
d_hat <- polls %>%
    summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>%
    .$d_hat

# compute margin of error
p_hat <- (d_hat+1)/2
moe <- 1.96 * 2 * sqrt(p_hat*(1-p_hat)/sum(polls$samplesize))

# histogram of the spread
polls %>%
    ggplot(aes(spread)) +
    geom_histogram(color="black", binwidth = .01)
```

*Code: Investigating poll data and pollster bias*

```{r}
# number of polls per pollster in week before election
polls %>% group_by(pollster) %>% summarize(n())

# plot results by pollsters with at least 6 polls
polls %>% group_by(pollster) %>%
    filter(n() >= 6) %>%
    ggplot(aes(pollster, spread)) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

# standard errors within each pollster
polls %>% group_by(pollster) %>%
    filter(n() >= 6) %>%
    summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))
```

## Data-Driven Models

The textbook for this section is available [here](https://rafalab.github.io/dsbook/models.html#data-driven-model)

**Key points**

* Instead of using an urn model where each poll is a random draw from the same distribution of voters, we instead define a model using an urn that contains poll results from all possible pollsters.
* We assume the expected value of this model is the actual spread $d = 2p - 1$.
* Our new standard error $\sigma$ now factors in pollster-to-pollster variability. It can no longer be calculated from $p$ or $d$ and is an unknown parameter.
* The central limit theorem still works to estimate the sample average of many polls $X_1,...,X_N$ because the average of the sum of many random variables is a normally distributed random variable with expected value $d$ and standard error $\sigma / \sqrt{N}$.
* We can estimate the unobserved $\sigma$ as the sample standard deviation, which is calculated with the ```sd``` function.

*Code* 

Note that to compute the exact 95% confidence interval, we would use ```qnorm(.975)``` instead of ```1.96```.

```{r}
# collect last result before the election for each pollster
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
    filter(enddate == max(enddate)) %>%      # keep latest poll
    ungroup()

# histogram of spread estimates
one_poll_per_pollster %>%
    ggplot(aes(spread)) + geom_histogram(binwidth = 0.01)

# construct 95% confidence interval
results <- one_poll_per_pollster %>%
    summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - 1.96*se, end = avg + 1.96*se)
round(results*100, 1)
```

## Assessment - Statistical Models

1. We have been using *urn models* to motivate the use of probability models. 

However, most data science applications are not related to data obtained from urns. More common are data that come from individuals. Probability plays a role because the data come from a random sample. The random sample is taken from a population and the urn serves as an analogy for the population.

Let's revisit the heights dataset. For now, consider ```x``` to be the heights of all males in the data set. Mathematically speaking, ```x``` is our population. Using the urn analogy, we have an urn with the values of ```x``` in it.

What are the population average and standard deviation of our population?

```{r}
data(heights)

# Make a vector of heights from all males in the population
x <- heights %>% filter(sex == "Male") %>%
  .$height

# Calculate the population average. Print this value to the console.
gemiddelde_lengte <- mean(x)
gemiddelde_lengte

# Calculate the population standard deviation. Print this value to the console.
standaard_deviatie <- sd(x)
standaard_deviatie
```

2. Call the population average computed above $\mu$ and the standard deviation $\sigma$. Now take a sample of size 50, with replacement, and construct an estimate for $\mu$ and $\sigma$.

```{r}
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)
X

# Calculate the sample average. Print this value to the console.
mu <- mean(X)
mu

# Calculate the sample standard deviation. Print this value to the console.
sigma <- sd(X)
sigma
```

3. What does the central limit theory tell us about the sample average and how it is related to $\mu$, the population average?

- [ ] A. It is identical to $\mu$.
- [X] B. It is a random variable with expected value $\mu$ and standard error $\sigma / \sqrt{N}$.
- [ ] C. It is a random variable with expected value $\mu$ and standard error $\sigma$.
- [ ] D. It underestimates $\mu$.

4. We will use $\overline{X}$ as our estimate of the heights in the population from our sample size $N$. We know from previous exercises that the standard estimate of our error $\overline{X} - \mu$ is $\sigma / \sqrt{N}$.

Construct a 95% confidence interval for $\mu$.

```{r}
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)

# Define `se` as the standard error of the estimate. Print this value to the console.
X_hat <- mean(X)
se <- sd(X)/sqrt(N)
se

# Construct a 95% confidence interval for the population average based on our sample. Save the lower and then the upper confidence interval to a variable called `ci`.
# Construct a 95% confidence interval for the population average based on our sample. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(qnorm(0.025, X_hat, se), qnorm(0.975, X_hat, se))
ci
```

5. Now run a Monte Carlo simulation in which you compute 10,000 confidence intervals as you have just done. 

What proportion of these intervals include $\mu$?

```{r}
# Define `mu` as the population average
mu <- mean(x)

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Define `N` as the number of people measured
N <- 50

# Define `B` as the number of times to run the model
B <- 10000

# Define an object `res` that contains a logical vector for simulated intervals that contain mu
res <- replicate(B, {
  X <- sample(x, N, replace = TRUE)
  se <- sd(X) / sqrt(N)
  interval <- c(qnorm(0.025, mean(X), se) , qnorm(0.975, mean(X), se))
  between(mu, interval[1], interval[2])
})

# Calculate the proportion of results in `res` that include mu. Print this value to the console.
mean(res)
```

6. In this section, we used visualization to motivate the presence of pollster bias in election polls. 

Here we will examine that bias more rigorously. Lets consider two pollsters that conducted daily polls and look at national polls for the month before the election.

Is there a poll bias? Make a plot of the spreads for each poll.

```{r}
# These lines of code filter for the polls we want and calculate the spreads
polls <- polls_us_election_2016 %>% 
  filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid") &
           enddate >= "2016-10-15" &
           state == "U.S.") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) 

# Make a boxplot with points of the spread for each pollster
polls %>% ggplot(aes(pollster, spread)) + geom_boxplot() + geom_point()
```

7. The data do seem to suggest there is a difference between the pollsters. 

However, these data are subject to variability. Perhaps the differences we observe are due to chance. Under the urn model, both pollsters should have the same expected value: the election day difference, $d$.

We will model the observed data $Y_{ij}$ in the following way:

$Y_{ij} = d + b_i + \varepsilon_{ij}$

with $i = 1, 2$ indexing the two pollsters, $b_i$ the bias for pollster $i$, and $\varepsilon_{ij}$ poll to poll chance variability. We assume the $\varepsilon$ are independent from each other, have expected value 0 and standard deviation $\sigma_i$ regardless of $j$.

Which of the following statements best reflects what we need to know to determine if our data fit the urn model?

- [ ] A. Is $\varepsilon_{ij} = 0$?
- [ ] B. How close are $Y_{ij}$ to $d$?
- [X] C. Is $b_1 \neq b_2$?
- [ ] D. Are $b_1 = 0$ and $b_2 = 0$?

8. We modelled the observed data $Y_{ij}$ as:

$Y_{ij} = d + b_i + \varepsilon_{ij}$

On the right side of this model, only $\varepsilon_{ij}$ is a random variable. The other two values are constants.

What is the expected value of $Y_{1j}$?

- [X] A.  $d + b_1$
- [ ] B.  $b_1 + \varepsilon_{ij}$
- [ ] C.  $d$
- [ ] D.  $d + b_1 + \varepsilon_{ij}$

9. Suppose we define $\overline{Y}_1$ as the average of poll results from the first poll and $\sigma_1$ as the standard deviation of the first poll.

What is the expected value and standard error of $\overline{Y}_1$?

- [ ] A. The expected value is $d + b_1$ and the standard error is $\sigma_1$
- [ ] B. The expected value is $d$ and the standard error is $\sigma_1/\sqrt{N_1}$
- [X] C. The expected value is $d + b_1$ and the standard error is $\sigma_1/\sqrt{N_1}$
- [ ] D. The expected value is $d$ and the standard error is $\sigma_1 + \sqrt{N_1}$

10. Now we define $\overline{Y}_2$ as the average of poll results from the second poll.

What is the expected value and standard error of $\overline{Y}_2$?

- [ ] A. The expected value is $d + b_2$ and the standard error is $\sigma_2$ 
- [ ] B. The expected value is $d$ and the standard error is $\sigma_2/\sqrt{N_2}$
- [X] C. The expected value is $d + b_2$ and the standard error is $\sigma_2/\sqrt{N_2}$
- [ ] D. The expected value is $d$ and the standard error is $\sigma_2 + \sqrt{N_2}$

11. Using what we learned by answering the previous questions, what is the expected value of $\overline{Y}_2 − \overline{Y}_1$?

- [ ] A.  $(b_2 − b_1)^2$
- [ ] B.  $b_2 − b_1/\sqrt(N)$
- [ ] C.  $b_2 + b_1$
- [X] D.  $b_2 − b_1$

12. Standard Error of the Difference Between Polls

Using what we learned by answering the questions above, what is the standard error of $\overline{Y}_2 − \overline{Y}_1$?

- [X] A.  $\sqrt{\sigma_2^2/N_2 + \sigma_1^2/N_1}$
- [ ] B.  $\sqrt{\sigma_2/N_2 + \sigma_1/N_1}$
- [ ] C.  $(\sigma_2^2/N_2 + \sigma_1^2/N_1)^2$ 
- [ ] D.  $\sigma_2^2/N_2 + \sigma_1^2/N_1$

13. The answer to the previous question depends on $\sigma_1$ and $\sigma_2$, which we don't know. 

We learned that we can estimate these values using the sample standard deviation.

Compute the estimates of $\sigma_1$ and $\sigma_2$.

```{r}
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)

# Create an object called `sigma` that contains a column for `pollster` and a column for `s`, the standard deviation of the spread
sigma <- polls %>% group_by(pollster) %>% summarize(s = sd(spread))

# Print the contents of sigma to the console
sigma
```

14. What does the central limit theorem tell us about the distribution of the differences between the pollster averages, $\overline{Y}_2 − \overline{Y}_1$?

- [ ] A. The central limit theorem cannot tell us anything because this difference is not the average of a sample.
- [ ] B. Because $Y_{ij}$ are approximately normal, the averages are normal too.
- [X] C. If we assume $N_2$ and $N_1$ are large enough, $\overline{Y}_2$ and $\overline{Y}_1$, and their difference, are approximately normal.
- [ ] D. These data do not contain vectors of 0 and 1, so the central limit theorem does not apply.

15. We have constructed a random variable that has expected value $b_2 − b_1$, the pollster bias difference. 

If our model holds, then this random variable has an approximately normal distribution. The standard error of this random variable depends on $\sigma_1$ and $\sigma_2$, but we can use the sample standard deviations we computed earlier. We have everything we need to answer our initial question: is $b_2 − b_1$ different from 0?

Construct a 95% confidence interval for the difference $b_2$ and $b_1$. Does this interval contain zero?

```{r}
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)

# Create an object called `res` that summarizes the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% summarise(avg = mean(spread), s = sd(spread), N=n())

# Store the difference between the larger average and the smaller in a variable called `estimate`. Print this value to the console.
estimate <- max(res$avg) - min(res$avg)
estimate

# Store the standard error of the estimates as a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
se_hat

# Calculate the 95% confidence interval of the spreads. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(estimate - qnorm(0.975)*se_hat, estimate + qnorm(0.975)*se_hat)
ci
```

16. The confidence interval tells us there is relatively strong pollster effect resulting in a difference of about 5%. 

Random variability does not seem to explain it.

Compute a p-value to relay the fact that chance does not explain the observed pollster effect.

```{r}
# We made an object `res` to summarize the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>% 
  summarize(avg = mean(spread), s = sd(spread), N = n()) 

# The variables `estimate` and `se_hat` contain the spread estimates and standard error, respectively.
estimate <- res$avg[2] - res$avg[1]
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])

# Calculate the p-value
2 * (1 - pnorm(estimate / se_hat, 0, 1))
```

17. We compute statistic called the *t-statistic* by dividing our estimate of $b_2 − b_1$ by its estimated standard error:

$\frac{\overline{Y_2} - \overline{Y_1}}{\sqrt{s_2^2/N_2 + s_1^2/N_1}}$

Later we learn will learn of another approximation for the distribution of this statistic for values of $N_2$ and $N_1$ that aren't large enough for the CLT.

Note that our data has more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?

Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.

```{r}
# Execute the following lines of code to filter the polling data and calculate the spread
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-15" &
           state == "U.S.") %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  ungroup()

# Create an object called `var` that contains columns for the pollster, mean spread, and standard deviation. Print the contents of this object to the console.
var <- polls %>% group_by(pollster) %>% summarise(avg = mean(spread), s = sd(spread))
var
```

## Section 5 Overview

In Section 5, you will learn about Bayesian statistics through looking at examples from rare disease diagnosis and baseball.

After completing Section 5, you will be able to:

* Apply Bayes’ theorem to calculate the probability of A given B.
* Understand how to use hierarchical models to make better predictions by considering multiple levels of variability.
* Compute a posterior probability using an empirical Bayesian approach.
* Calculate a 95% credible interval from a posterior probability.

## Bayesian Statistics

The textbook for this section is available [here](https://rafalab.github.io/dsbook/models.html#bayesian-statistics)

**Key points**

* In the urn model, it does not make sense to talk about the probability of $p$ being greater than a certain value because $p$ is a fixed value.
* With Bayesian statistics, we assume that $p$ is in fact random, which allows us to calculate probabilities related to $p$.
* Hierarchical models describe variability at different levels and incorporate all these levels into a model for estimating $p$.

## Bayes' Theorem



## Assessment 5.1: Bayesian Statistics

Statistics in the Courtroom

In 1999 in England Sally Clark was found guilty of the murder of two of her sons. Both infants were found dead in the morning, one in 1996 and another in 1998, and she claimed the cause of death was sudden infant death syndrome (SIDS). No evidence of physical harm was found on the two infants so the main piece of evidence against her was the testimony of Professor Sir Roy Meadow, who testified that the chances of two infants dying of SIDS was 1 in 73 million. He arrived at this figure by finding that the rate of SIDS was 1 in 8,500 and then calculating that the chance of two SIDS cases was 8,500 × 8,500 ≈ 73 million.

Based on what we’ve learned throughout this course, which statement best describes a potential flaw in Sir Meadow’s reasoning?

Possible Answers
- [X] A. Sir Meadow assumed the second death was independent of the first son being affected, thereby ignoring possible genetic causes.
- [ ] B. There is no flaw. The multiplicative rule always applies in this way: Pr(A and B)=Pr(A)Pr(B)
- [ ] C. Sir Meadow should have added the probabilities: Pr(A and B)=Pr(A)+Pr(B)
- [ ] D. The rate of SIDS is too low to perform these types of statistics.

2. Recalculating the SIDS Statistics

Let’s assume that there is in fact a genetic component to SIDS and the the probability of Pr(second case of SIDS∣first case of SIDS) = 1/100, is much higher than 1 in 8,500.

What is the probability of both of Sally Clark’s sons dying of SIDS?
Instructions
- Calculate the probability of both sons dying to SIDS.
```
# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1/8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1/100

# Calculate the probability of both sons dying of SIDS. Print this value to the console.
Pr_1*Pr_2
```
```
## [1] 1.176471e-06
```
3. NBayes’ Rule in the Courtroom

Many press reports stated that the expert claimed the probability of Sally Clark being innocent as 1 in 73 million. Perhaps the jury and judge also interpreted the testimony this way. This probability can be written like this:

Pr(mother is a murderer∣two children found dead with no evidence of harm)

Possible Answers
- [ ] A.  Pr(two children found dead with no evidence of harm)Pr(mother is a murderer)/Pr(two children found dead with no evidence of harm)
- [ ] B.  Pr(two children found dead with no evidence of harm)Pr(mother is a murderer)
- [X] C.  Pr(two children found dead with no evidence of harm∣mother is a murderer)Pr(mother is a murderer)/Pr(two children found dead with no evidence of harm)
- [ ] D. 1/8500

4. Calculate the Probability

Assume that the probability of a murderer finding a way to kill her two children without leaving evidence of physical harm is:

Pr(two children found dead with no evidence of harm∣mother is a murderer)=0.50

Assume that the murder rate among mothers is 1 in 1,000,000.

Pr(mother is a murderer)=1/1,000,000

According to Bayes’ rule, what is the probability of:

Pr(mother is a murderer∣two children found dead with no evidence of harm)

Instructions
- Use Bayes’ rule to calculate the probability that the mother is a murderer, considering the rates of murdering mothers in the population, the probability that two siblings die of SIDS, and the probability that a murderer kills children without leaving evidence of physical harm.
```
# Define `Pr_1` as the probability of the first son dying of SIDS
Pr_1 <- 1/8500

# Define `Pr_2` as the probability of the second son dying of SIDS
Pr_2 <- 1/100

# Define `Pr_B` as the probability of both sons dying of SIDS
Pr_B <- Pr_1*Pr_2

# Define Pr_A as the rate of mothers that are murderers
Pr_A <- 1/1000000

# Define Pr_BA as the probability that two children die without evidence of harm, given that their mother is a murderer
Pr_BA <- 0.50

# Define Pr_AB as the probability that a mother is a murderer, given that her two children died with no evidence of physical harm. Print this value to the console.
Pr_AB <- Pr_BA*Pr_A/Pr_B
Pr_AB
```
```
## [1] 0.425
```
5. Misuse of Statistics in the Courts

After Sally Clark was found guilty, the Royal Statistical Society issued a statement saying that there was “no statistical basis” for the expert’s claim. They expressed concern at the “misuse of statistics in the courts”. Eventually, Sally Clark was acquitted in June 2003.

In addition to misusing the multiplicative rule as we saw earlier, what else did Sir Meadow miss?

Possible Answers
- [ ] A. He made an arithmetic error in forgetting to divide by the rate of SIDS in siblings.
- [X] B. He did not take into account how rare it is for a mother to murder her children.
- [ ] C. He mixed up the numerator and denominator of Bayes’ rule.
- [ ] D. He did not take into account murder rates in the population.

6. Back to Election Polls
    
Florida is one of the most closely watched states in the U.S. election because it has many electoral votes and the election is generally close. Create a table with the poll spread results from Florida taken during the last days before the election using the sample code.

The CLT tells us that the average of these spreads is approximately normal. Calculate a spread average and provide an estimate of the standard error.

Instructions
- Calculate the average of the spreads. Call this average avg in the final table.
- Calculate an estimate of the standard error of the spreads. Call this standard error se in the final table.
- Use the mean and sd functions nested within summarize to find the average and standard deviation of the grouped spread data.
- Save your results in an object called results.
```
# Load the libraries and poll data
library(dplyr)
library(dslabs)
data(polls_us_election_2016)

# Create an object `polls` that contains the spread of predictions for each candidate in Florida during the last polling days
polls <- polls_us_election_2016 %>% 
  filter(state == "Florida" & enddate >= "2016-11-04" ) %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# Examine the `polls` object using the `head` function
head(polls)
```
```
   state    startdate       endddate        pollster                                        grade
   <fctr>   <date>          <date>          <fctr>                                          <fctr>
1  Florida  2016-11-03	    2016-11-06	    Quinnipiac University	                    A-	
2  Florida  2016-11-02	    2016-11-04	    YouGov	                                    B	
3  Florida  2016-11-01	    2016-11-07	    SurveyMonkey	                            C-	
4  Florida  2016-11-06	    2016-11-06	    Trafalgar Group	                            C	
5  Florida  2016-11-05      2016-11-06	    Opinion Savvy/InsiderAdvantage	            C-	
6  Florida  2016-11-02      2016-11-06	    Rasmussen Reports/Pulse Opinion Research	    C+	
6 rows | 1-6 of 17 columns
```
```
# Create an object called `results` that has two columns containing the average spread (`avg`) and the standard error (`se`). Print the results to the console.
results <- polls %>% summarize(avg = mean(spread),  se = sd(spread)/sqrt(n()))
results
```
```
avg            se
<dbl>          <dbl>
0.004154545    0.007218692
1 row
```
7. The Prior Distribution

Assume a Bayesian model sets the prior distribution for Florida’s election night spread d to be normal with expected value μ and standard deviation τ.

What are the interpretations of μ and τ?

Possible Answers
- [ ] A. μ and τ are arbitrary numbers that let us make probability statements about d.
- [X] B. μ and τ summarize what we would predict for Florida before seeing any polls.
- [ ] C. μ and τ summarize what we want to be true. We therefore set μ at 0.10 and τ at 0.01.
- [ ] D. The choice of prior has no effect on the Bayesian analysis.

8. Estimate the Posterior Distribution

The CLT tells us that our estimate of the spread ^d has a normal distribution with expected value d and standard deviation σ, which we calculated in a previous exercise.

Use the formulas for the posterior distribution to calculate the expected value of the posterior distribution if we set μ=0 and τ=0.01.

Instructions
- Define μ and τ Identify which elements stored in the object results represent σ and Y
- Estimate B using σ and τ
- Estimate the posterior distribution using B, μ, and Y
```
# The results` object has already been loaded. Examine the values stored: `avg` and `se` of the spread
results
```
```
avg             se
<dbl>           <dbl>
0.004154545	0.007218692
1 row
```
```
# Define `mu` and `tau`
mu <- 0
tau <- 0.01

# Define a variable called `sigma` that contains the standard error in the object `results
sigma <- results$se

# Define a variable called `Y` that contains the average in the object `results`
Y <- results$avg

# Define a variable `B` using `sigma` and `tau`. Print this value to the console.
tau <- 0.01
miu <- 0
B <- sigma^2 / (sigma^2 + tau^2)
B
```
```
## [1] 0.342579
```
```
# Calculate the expected value of the posterior distribution
miu + (1 - B) * (Y - miu)
```
```
## [1] 0.002731286
```
9. Standard Error of the Posterior Distribution

Compute the standard error of the posterior distribution.

Instructions
- Using the variables we have defined so far, calculate the standard error of the posterior distribution.
- Print this value to the console.
```
# Here are the variables we have defined
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

# Compute the standard error of the posterior distribution. Print this value to the console.
sqrt(1 / (1 / sigma ^2 + 1 / tau ^2))
```
```
## [1] 0.005853024
```
10. Constructing a Credible Interval

Using the fact that the posterior distribution is normal, create an interval that has a 95% of occurring centered at the posterior expected value. Note that we call these credible intervals.

Instructions
- Calculate the 95% credible intervals using the qnorm function.
- Save the lower and upper confidence intervals as an object called ci. Save the lower - confidence interval first.
```
# Here are the variables we have defined in previous exercises
mu <- 0
tau <- 0.01
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

# Construct the 95% credible interval. Save the lower and then the upper confidence interval to a variable called `ci`.
est <- B * mu + (1 - B) * Y
est
```
```
## [1] 0.002731286
```
```
ci <- c(est - qnorm(0.975) * se, est + qnorm(0.975) * se)
ci
```
```
## [1] -0.008740432  0.014203003
```
11. Odds of Winning Florida

According to this analysis, what was the probability that Trump wins Florida?

Instructions
- Using the pnorm function, calculate the probability that the spread in Florida was less than 0.
```
# Assign the expected value of the posterior distribution to the variable `exp_value`
exp_value <- B*mu + (1-B)*Y 

# Assign the standard error of the posterior distribution to the variable `se`
se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

# Using the `pnorm` function, calculate the probability that the actual spread was less than 0 (in Trump's favor). Print this value to the console.
pnorm(0, exp_value, se)
```
```
## [1] 0.3203769
```
12. Change the Priors

We had set the prior variance τ to 0.01, reflecting that these races are often close.

Change the prior variance to include values ranging from 0.005 to 0.05 and observe how the probability of Trump winning Florida changes by making a plot.

Instructions
- Create a vector of values of taus by executing the sample code.
- Create a function using function(){} called p_calc that first calculates B given tau and sigma and then calculates the probability of Trump winning, as we did in the previous exercise.
- Apply your p_calc function across all the new values of taus.
- Use the plot function to plot τ on the x-axis and the new probabilities on the y-axis.
```
# Define the variables from previous exercises
mu <- 0
sigma <- results$se
Y <- results$avg

# Define a variable `taus` as different values of tau
taus <- seq(0.005, 0.05, len = 100)

# Create a function called `p_calc` that generates `B` and calculates the probability of the spread being less than 0
p_calc <- function(tau) {
  B <- sigma ^ 2 / (sigma^2 + tau^2)
  se <- sqrt(1 / (1/sigma^2 + 1/tau^2))
  exp_value <- B * mu + (1 - B) * Y
  pnorm(0, exp_value, se)
}

# Create a vector called `ps` by applying the function `p_calc` across values in `taus`
ps <- p_calc(taus)

# Plot `taus` on the x-axis and `ps` on the y-axis
plot(taus, ps)
```

## Section 6 Overview

In Section 6, you will learn about election forecasting, building on what you’ve learned in the previous sections about statistical modeling and Bayesian statistics.

After completing Section 6, you will be able to:
- Understand how pollsters use hierarchical models to forecast the results of elections.
- Incorporate multiple sources of variability into a mathematical model to make predictions.
- Construct confidence intervals that better model deviations such as those seen in election data using the t-distribution. 

There are 2 assignments that use the DataCamp platform for you to practice your coding skills.

The textbook for this section is available [here](https://rafalab.github.io/dsbook/models.html#election-forecasting)

## Assessment 6.1: Election Forecasting

1. Confidence Intervals of Polling Data

For each poll in the polling data set, use the CLT to create a 95% confidence interval for the spread. Create a new table called cis that contains columns for the lower and upper limits of the confidence intervals.

Instructions
- Use pipes %>% to pass the poll object on to the mutate function, which creates new variables.
- Create a variable called X_hat that contains the estimate of the proportion of Clinton voters for each poll.
- Create a variable called se that contains the standard error of the spread.
- Calculate the confidence intervals using the qnorm function and your calculated se.
- Use the select function to keep the following columns: state, startdate, enddate, pollster, grade, spread, lower, upper.
```
## Load the libraries and data
library(dplyr)
library(dslabs)
data("polls_us_election_2016")

# Create a table called `polls` that filters by  state, date, and reports the spread
polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# Create an object called `cis` that has the columns indicated in the instructions
cis <- polls %>% mutate(X_hat = (spread+1)/2, se = 2*sqrt(X_hat*(1-X_hat)/samplesize), 
                 lower = spread - qnorm(0.975)*se, upper = spread + qnorm(0.975)*se) %>%
  select(state, startdate, enddate, pollster, grade, spread, lower, upper)
```
2. Compare to Actual Results

You can add the final result to the cis table you just created using the left_join function as shown in the sample code.

Now determine how often the 95% confidence interval includes the actual result.

Instructions
- Create an object called p_hits that contains the proportion of intervals that contain the actual spread using the following steps.
- Use the mutate function to create a new variable called hit that contains a logical vector for whether the actual_spread falls between the lower and upper confidence intervals.
- Summarize the proportion of values in hit that are true as a variable called proportion_hits.
```
# Add the actual results to the `cis` data set
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of confidence intervals that contain the actual value. Print this object to the console.
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% summarize(proportion_hits = mean(hit))
p_hits
```
```
                                 proportion_hits
                                 <dbl>
                                 0.66133
1 row
```
3. Stratify by Pollster and Grade

Now find the proportion of hits for each pollster. Show only pollsters with more than 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster.

Instructions
- Create an object called p_hits that contains the proportion of intervals that contain the actual spread using the following steps.
- Use the mutate function to create a new variable called hit that contains a logical vector for whether the actual_spread falls between the lower and upper confidence intervals.
- Use the group_by function to group the data by pollster.
- Use the filter function to filter for pollsters that have more than 5 polls.
- Summarize the proportion of values in hit that are true as a variable called proportion_hits. Also create new variables for the number of polls by each pollster using the n() function and the grade of each poll.
- Use the arrange function to arrange the proportion_hits in descending order.
```
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each pollster that has at least 5 polls. 
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% 
  group_by(pollster) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n(), grade = grade[1]) %>%
  arrange(desc(proportion_hits))
p_hits
```
```
pollster                                     proportion_hits        n            grade
<fctr>                                       <dbl>                  <int>        <fctr>
Quinnipiac University	                     1.0000000	            6	         A-
Emerson College	                             0.9090909	            11	         B
Public Policy Polling	                     0.8888889	            9	         B+
University of New Hampshire	             0.8571429	            7	         B+
Ipsos	                                     0.8067227	            119	         A-
Mitchell Research & Communications	     0.8000000	            5	         D
Gravis Marketing	                     0.7826087	            23	         B-
Trafalgar Group	                             0.7777778	            9	         C
Rasmussen Reports/Pulse Opinion Research     0.7741935	            31	         C+
Remington	                             0.6666667	            9	         NA
1-10 of 13 rows
```
4. Stratify by State

Repeat the previous exercise, but instead of pollster, stratify by state. Here we can’t show grades.

Instructions
- Create an object called p_hits that contains the proportion of intervals that contain the actual spread using the following steps.
- Use the mutate function to create a new variable called hit that contains a logical vector for whether the actual_spread falls between the lower and upper confidence intervals.
- Use the group_by function to group the data by state.
- Use the filter function to filter for states that have more than 5 polls.
- Summarize the proportion of values in hit that are true as a variable called proportion_hits. Also create new variables for the number of polls in each state using the n() function.
- Use the arrange function to arrange the proportion_hits in descending order.
```
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each state that has more than 5 polls. 
p_hits <- ci_data %>% mutate(hit = lower <= actual_spread & upper >= actual_spread) %>% 
  group_by(state) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n()) %>%
  arrange(desc(proportion_hits)) 
p_hits
```
```
state           proportion_hits        n
<chr>           <dbl>                  <int>
Connecticut	1.0000000	       13
Delaware	1.0000000	       12
Rhode Island	1.0000000	       10
New Mexico	0.9411765	       17
Washington	0.9333333	       15
Oregon	        0.9285714	       14
Illinois	0.9230769	       13
Nevada	        0.9230769	       26
Maine	        0.9166667	       12
Montana	        0.9166667	       12
1-10 of 51 rows
```
5. Plotting Prediction Results

Make a barplot based on the result from the previous exercise.

Instructions
- Reorder the states in order of the proportion of hits.
- Using ggplot, set the aesthetic with state as the x-variable and proportion of hits as the y-variable.
- Use geom_bar to indicate that we want to plot a barplot. Specifcy stat = "identity" to indicate that the height of the bar should match the value.
- Use coord_flip to flip the axes so the states are displayed from top to bottom and proportions are displayed from left to right.
```
# The `p_hits` data have already been loaded for you. Use the `head` function to examine it.
head(p_hits)
```
```
state          proportion_hits    n
<chr>          <dbl>              <int>
Connecticut    1.0000000	  13
Delaware       1.0000000	  12
Rhode Island   1.0000000	  10
New Mexico     0.9411765	  17
Washington     0.9333333	  15
Oregon	       0.9285714	  14
6 rows
```
```
# Make a barplot of the proportion of hits for each state
p_hits %>% mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(state, proportion_hits)) + 
  geom_bar(stat = "identity") +
  coord_flip()
```

6. Predicting the Winner

Even if a forecaster’s confidence interval is incorrect, the overall predictions will do better if they correctly called the right winner.

Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same.

Instructions
- Use the mutate function to add two new variables to the cis object: error and hit.
- For the error variable, subtract the actual spread from the spread.
- For the hit variable, return “TRUE” if the poll predicted the actual winner.
- Save the new table as an object called errors.
- Use the tail function to examine the last 6 rows of errors```.
```
# The `cis` data have already been loaded. Examine it using the `head` function.

cis <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

head(cis)
```
```
      state            startdate       enddate        pollster                 grade          spread
      <chr>            <date>          <date>         <fctr>                   <fctr>         <dbl>
1     New Mexico       2016-11-06      2016-11-06     Zia Poll	               NA	      0.02	
2     Virginia	       2016-11-03      2016-11-04     Public Policy Polling    B+	      0.05	
3     Iowa	       2016-11-01      2016-11-04     Selzer & Company	       A+	     -0.07	
4     Wisconsin	       2016-10-26      2016-10-31     Marquette University     A	      0.06	
5     North Carolina   2016-11-04      2016-11-06     Siena College	       A	      0.00	
6     Georgia	       2016-11-06      2016-11-06     Landmark Communications  B	     -0.03	
6 rows | 1-7 of 10 columns
```
```
# Create an object called `errors` that calculates the difference between the predicted and actual spread and indicates if the correct winner was predicted
errors <- cis %>% mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))

# Examine the last 6 rows of `errors`
tail(errors)
```
```
      state            startdate       enddate        pollster                 grade          spread
      <chr>            <date>          <date>         <fctr>                   <fctr>         <dbl>
807   Utah	       2016-10-04      2016-11-06     YouGov	               B	     -0.0910	
808   Utah	       2016-10-25      2016-10-31     Google Consumer Surveys  B	     -0.0121	
809   South Dakota     2016-10-28      2016-11-02     Ipsos	               A-	     -0.1875	
810   Washington       2016-10-21      2016-11-02     Ipsos	               A-	      0.0838	
811   Utah	       2016-11-01      2016-11-07     Google Consumer Surveys  B	     -0.1372	
812   Oregon	       2016-10-21      2016-11-02     Ipsos	               A-	      0.0905	
6 rows | 1-7 of 12 columns
```
7. Plotting Prediction Results

Create an object called p_hits that contains the proportion of instances when the sign of the actual spread matches the predicted spread for states with more than 5 polls.

Make a barplot based on the result from the previous exercise that shows the proportion of times the sign of the spread matched the actual result for the data in p_hits.

Instructions
- Use the group_by function to group the data by state.
- Use the filter function to filter for states that have more than 5 polls.
- Summarize the proportion of values in hit that are true as a variable called proportion_hits. Also create new variables for the number of polls in each state using the n() function.
- To make the plot, follow these steps:
- Reorder the states in order of the proportion of hits.
- Using ggplot, set the aesthetic with state as the x-variable and proportion of hits as the y-variable.
- Use geom_bar to indicate that we want to plot a barplot.
- Use coord_flip to flip the axes so the states are displayed from top to bottom and proportions are displayed from left to right.
```
# Create an object called `errors` that calculates the difference between the predicted and actual spread and indicates if the correct winner was predicted
errors <- cis %>% mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))

# Create an object called `p_hits` that summarizes the proportion of hits for each state that has more than 5 polls
p_hits <- errors %>%  group_by(state) %>%
  filter(n() >=  5) %>%
  summarize(proportion_hits = mean(hit), n = n())

# Make a barplot of the proportion of hits for each state
p_hits %>% mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(state, proportion_hits)) + 
  geom_bar(stat = "identity") +
 coord_flip()
```

8. Plotting the Errors

In the previous graph, we see that most states’ polls predicted the correct winner 100% of the time. Only a few states polls’ were incorrect more than 25% of the time. Wisconsin got every single poll wrong. In Pennsylvania and Michigan, more than 90% of the polls had the signs wrong.

Make a histogram of the errors. What is the median of these errors?

Instructions
- Use the hist function to generate a histogram of the errors
- Use the median function to compute the median error
```
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)
```
```
      state            startdate       enddate        pollster                 grade          spread
      <chr>            <date>          <date>         <fctr>                   <fctr>         <dbl>
1     New Mexico       2016-11-06      2016-11-06     Zia Poll	               NA	      0.02	
2     Virginia	       2016-11-03      2016-11-04     Public Policy Polling    B+	      0.05	
3     Iowa	       2016-11-01      2016-11-04     Selzer & Company	       A+	     -0.07	
4     Wisconsin	       2016-10-26      2016-10-31     Marquette University     A	      0.06	
5     North Carolina   2016-11-04      2016-11-06     Siena College	       A	      0.00	
6     Georgia	       2016-11-06      2016-11-06     Landmark Communications  B	     -0.03	
6 rows | 1-7 of 12 columns
```
```
# Generate a histogram of the error
hist(errors$error)
```

```
# Calculate the median of the errors. Print this value to the console.
median(errors$error)
```
```
## [1] 0.037
```
9. Plot Bias by State

We see that, at the state level, the median error was slightly in favor of Clinton. The distribution is not centered at 0, but at 0.037. This value represents the general bias we described in an earlier section.

Create a boxplot to examine if the bias was general to all states or if it affected some states differently. Filter the data to include only pollsters with grades B+ or higher.

Instructions
- Use the filter function to filter the data for polls with grades equal to A+, A, A-, or B+.
- Use the reorder function to order the state data by error.
- Using ggplot, set the aesthetic with state as the x-variable and error as the y-variable.
- Use geom_boxplot to indicate that we want to plot a boxplot.
- Use geom_point to add data points as a layer.
```
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)
```
```
      state            startdate       enddate        pollster                 grade          spread
      <chr>            <date>          <date>         <fctr>                   <fctr>         <dbl>
1     New Mexico       2016-11-06      2016-11-06     Zia Poll	               NA	      0.02	
2     Virginia	       2016-11-03      2016-11-04     Public Policy Polling    B+	      0.05	
3     Iowa	       2016-11-01      2016-11-04     Selzer & Company	       A+	     -0.07	
4     Wisconsin	       2016-10-26      2016-10-31     Marquette University     A	      0.06	
5     North Carolina   2016-11-04      2016-11-06     Siena College	       A	      0.00	
6     Georgia	       2016-11-06      2016-11-06     Landmark Communications  B	     -0.03	
6 rows | 1-7 of 12 columns
```
```
# Create a boxplot showing the errors by state for polls with grades B+ or higher
errors %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_boxplot() + 
  geom_point()
```

10. Filter Error Plot

Some of these states only have a few polls. Repeat the previous exercise to plot the errors for each state, but only include states with five good polls or more.

Instructions
- Use the filter function to filter the data for polls with grades equal to A+, A, A-, or B+.
- Group the filtered data by state using group_by.
- Use the filter function to filter the data for states with at least 5 polls.
- Use the reorder function to order the state data by error.
- Using ggplot, set the aesthetic with state as the x-variable and error as the y-variable.
- Use geom_box to indicate that we want to plot a boxplot.
- Use geom_point to add data points as a layer.
```
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)
```
```
      state            startdate       enddate        pollster                 grade          spread
      <chr>            <date>          <date>         <fctr>                   <fctr>         <dbl>
1     New Mexico       2016-11-06      2016-11-06     Zia Poll	               NA	      0.02	
2     Virginia	       2016-11-03      2016-11-04     Public Policy Polling    B+	      0.05	
3     Iowa	       2016-11-01      2016-11-04     Selzer & Company	       A+	     -0.07	
4     Wisconsin	       2016-10-26      2016-10-31     Marquette University     A	      0.06	
5     North Carolina   2016-11-04      2016-11-06     Siena College	       A	      0.00	
6     Georgia	       2016-11-06      2016-11-06     Landmark Communications  B	     -0.03	
6 rows | 1-7 of 12 columns
```
```
# Create a boxplot showing the errors by state for states with at least 5 polls with grades B+ or higher
errors %>% filter(grade %in% c("A+","A","A-","B+") | is.na(grade)) %>%
  group_by(state) %>%
  filter(n() >= 5) %>%
  ungroup() %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_boxplot() + 
  geom_point()
```

## Assessment 6.2: The t-Distribution

1. Using the t-Distribution

We know that, with a normal distribution, only 5% of values are more than 2 standard deviations away from the mean.

Calculate the probability of seeing t-distributed random variables being more than 2 in absolute value when the degrees of freedom are 3.

Instructions
- Use the pt function to calculate the probability of seeing a value less than or equal to the argument.
```
# Calculate the probability of seeing t-distributed random variables being more than 2 in absolute value when 'df = 3'.
1 - pt(2, 3) + pt(-2, 3)
```
```
## [1] 0.139326
```
2. Plotting the t-distribution

Now use sapply to compute the same probability for degrees of freedom from 3 to 50.

Make a plot and notice when this probability converges to the normal distribution’s 5%.

Instructions
- Make a vector called df that contains a sequence of numbers from 3 to 50.
- Using function, make a function called pt_func that recreates the calculation for the probability that a value is greater than 2 as an absolute value for any given degrees of freedom.
- Use sapply to apply the pt_func function across all values contained in df. Call these probabilities probs.
- Use the plot function to plot df on the x-axis and probs on the y-axis.
```
# Generate a vector 'df' that contains a sequence of numbers from 3 to 50
df <- seq(3,50)

# Make a function called 'pt_func' that calculates the probability that a value is more than |2| for any degrees of freedom 
pt_func <- function(n) {
  1 - pt(2, n) + pt(-2, n)
}

# Generate a vector 'probs' that uses the `pt_func` function to calculate the probabilities
probs <- sapply(df, pt_func)

# Plot 'df' on the x-axis and 'probs' on the y-axis
plot(df, probs)
```

3. Sampling From the Normal Distribution

In a previous section, we repeatedly took random samples of 50 heights from a distribution of heights. We noticed that about 95% of the samples had confidence intervals spanning the true population mean.

Re-do this Monte Carlo simulation, but now instead of N=50, use N=15. Notice what happens to the proportion of hits.

Instructions
- Use the replicate function to carry out the simulation. Specify the number of times you want the code to run and, within brackets, the three lines of code that should run.
- First use the sample function to randomly sample N values from x.
- Second, create a vector called interval that calculates the 95% confidence interval for the sample. You will use the qnorm function.
- Third, use the between function to determine if the population mean mu is contained between the confidence intervals.
- Save the results of the Monte Carlo function to a vector called res.
- Use the mean function to determine the proportion of hits in res.
```
# Load the neccessary libraries and data
library(dslabs)
library(dplyr)
data(heights)

# Use the sample code to generate 'x', a vector of male heights
x <- heights %>% filter(sex == "Male") %>%
  .$height

# Create variables for the mean height 'mu', the sample size 'N', and the number of times the simulation should run 'B'
mu <- mean(x)
N <- 15
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Generate a logical vector 'res' that contains the results of the simulations
res <- replicate(B, {
  X <- sample(x, N, replace=TRUE)
  interval <- mean(X) + c(-1,1)*qnorm(0.975)*sd(X)/sqrt(N)
  between(mu, interval[1], interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95% confidence interval. Print this value to the console.
mean(res)
```
```
## [1] 0.9331
```
4. Sampling from the t-Distribution

N=15 is not that big. We know that heights are normally distributed, so the t-distribution should apply. Repeat the previous Monte Carlo simulation using the t-distribution instead of using the normal distribution to construct the confidence intervals.

What are the proportion of 95% confidence intervals that span the actual mean height now?

Instructions
- Use the replicate function to carry out the simulation. Specify the number of times you want the code to run and, within brackets, the three lines of code that should run.
- First use the sample function to randomly sample N values from x.
- Second, create a vector called interval that calculates the 95% confidence interval for the sample. Remember to use the qt function this time to generate the confidence interval.
- Third, use the between function to determine if the population mean mu is contained between the confidence intervals.
- Save the results of the Monte Carlo function to a vector called res.
- Use the mean function to determine the proportion of hits in res.
```
# The vector of filtered heights 'x' has already been loaded for you. Calculate the mean.
mu <- mean(x)

# Use the same sampling parameters as in the previous exercise.
set.seed(1)
N <- 15
B <- 10000

# Generate a logical vector 'res' that contains the results of the simulations using the t-distribution
res <- replicate(B, {
  s <- sample(x, N, replace = TRUE)
  interval <- c(mean(s) - qt(0.975, N - 1) * sd(s) / sqrt(N), mean(s) + qt(0.975, N - 1) * sd(s) / sqrt(N))
  between(mu, interval[1], interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95% confidence interval. Print this value to the console.
mean(res)
```
```
## [1] 0.9512
```
5. Why the t-Distribution?

Why did the t-distribution confidence intervals work so much better?

Possible Answers
- [X] A. The t-distribution takes the variability into account and generates larger confidence intervals. 
- [ ] B. Because the t-distribution shifts the intervals in the direction towards the actual mean. 
- [ ] C. This was just a chance occurrence. If we run it again, the CLT will work better. 
- [ ] D. The t-distribution is always a better approximation than the normal distribution.

## Section 7 Overview

In Section 7, you will learn how to use association and chi-squared tests to perform inference for binary, categorical, and ordinal data through an example looking at research funding rates.

After completing Section 7, you will be able to:
- Use association and chi-squared tests to perform inference on binary, categorical, and ordinal data.
- Calculate an odds ratio to get an idea of the magnitude of an observed effect.

The textbook for this section is available [here](https://rafalab.github.io/dsbook/inference.html#association-tests)

## Assessment 7.1: Association and Chi-Squared Tests

1. Comparing Proportions of Hits

In a previous exercise, we determined whether or not each poll predicted the correct winner for their state in the 2016 U.S. presidential election. Each poll was also assigned a grade by the poll aggregator. Now we’re going to determine if polls rated A- made better predictions than polls rated C-.

In this exercise, filter the errors data for just polls with grades A- and C-. Calculate the proportion of times each grade of poll predicted the correct winner.

Instructions
- Filter errors for grades A- and C-.
- Group the data by grade and hit.
- Summarize the number of hits for each grade.
- Generate a two-by-two table containing the number of hits and misses for each grade.
- Calculate the proportion of times each grade was correct.
```
library(tidyr)
# The 'errors' data have already been loaded. Examine them using the `head` function.
head(errors)
```
```
      state            startdate       enddate        pollster                 grade          spread
      <chr>            <date>          <date>         <fctr>                   <fctr>         <dbl>
1     New Mexico       2016-11-06      2016-11-06     Zia Poll	               NA	      0.02	
2     Virginia	       2016-11-03      2016-11-04     Public Policy Polling    B+	      0.05	
3     Iowa	       2016-11-01      2016-11-04     Selzer & Company	       A+	     -0.07	
4     Wisconsin	       2016-10-26      2016-10-31     Marquette University     A	      0.06	
5     North Carolina   2016-11-04      2016-11-06     Siena College	       A	      0.00	
6     Georgia	       2016-11-06      2016-11-06     Landmark Communications  B	     -0.03	
6 rows | 1-7 of 12 columns
```
```
# Generate an object called 'totals' that contains the numbers of good and bad predictions for polls rated A- and C-
totals <- errors %>%
  filter(grade %in% c("A-", "C-")) %>%
  group_by(grade,hit) %>%
  summarize(num = n()) %>%
  spread(grade, num)

# Print the proportion of hits for grade A- polls to the console
totals[[2,3]]/sum(totals[[3]])
```
```
## [1] 0.8030303
```
```
# Print the proportion of hits for grade C- polls to the console
totals[[2,2]]/sum(totals[[2]])
```
```
## [1] 0.8614958
```
2. Chi-squared Test

We found that the A- polls predicted the correct winner about 86% of the time in their states and C- polls predicted the correct winner about 80% of the time.

Use a chi-squared test to determine if these proportions are different.

Instructions
- Use the chisq.test function to perform the chi-squared test. Save the results to an object called chisq_test.
- Print the p-value of the test to the console.
```
# The 'totals' data have already been loaded. Examine them using the `head` function.
head(totals)
```
```
hit     C-        A-
<lgl>   <int>     <int>
FALSE	50	  26
TRUE	311	  106
2 rows
```
```
# Perform a chi-squared test on the hit data. Save the results as an object called 'chisq_test'.
chisq_test <- totals %>% 
  select(-hit) %>%
  chisq.test()
chisq_test
```
```
## 
##  Pearson's Chi-squared test with Yates' continuity correction
## 
## data:  .
## X-squared = 2.1053, df = 1, p-value = 0.1468
```
```
# Print the p-value of the chi-squared test to the console
chisq_test$p.value
```
```
## [1] 0.1467902
```
3. Odds Ratio Calculation

It doesn’t look like the grade A- polls performed significantly differently than the grade C- polls in their states.

Calculate the odds ratio to determine the magnitude of the difference in performance between these two grades of polls.

Instructions
- Calculate the odds that a grade C- poll predicts the correct winner. Save this result to a variable called odds_C.
- Calculate the odds that a grade A- poll predicts the correct winner. Save this result to a variable called odds_A. 
- Calculate the odds ratio that tells us how many times larger the odds of a grade A- poll is at predicting the winner than a grade C- poll.
```
# The 'totals' data have already been loaded. Examine them using the `head` function.
head(totals)
```
```
hit     C-        A-
<lgl>   <int>     <int>
FALSE	50	  26
TRUE	311	  106
2 rows
```
```
# Generate a variable called `odds_C` that contains the odds of getting the prediction right for grade C- polls
odds_C <- (totals[[2,2]] / sum(totals[[2]])) / 
  (totals[[1,2]] / sum(totals[[2]]))

# Generate a variable called `odds_A` that contains the odds of getting the prediction right for grade A- polls
odds_A <- (totals[[2,3]] / sum(totals[[3]])) / 
  (totals[[1,3]] / sum(totals[[3]]))

# Calculate the odds ratio to determine how many times larger the odds ratio is for grade A- polls than grade C- polls
odds_A/odds_C
```
```
## [1] 0.6554539
```
4. Significance

We did not find meaningful differences between the poll results from grade A- and grade C- polls in this subset of the data, which only contains polls for about a week before the election. Imagine we expanded our analysis to include all election polls and we repeat our analysis. In this hypothetical scenario, we get that the p-value for the difference in prediction success if 0.0015 and the odds ratio describing the effect size of the performance of grade A- over grade B- polls is 1.07.

Based on what we learned in the last section, which statement reflects the best interpretation of this result?

Possible Answers
- [ ] A. The p-value is below 0.05, so there is a significant difference. Grade A- polls are significantly better at predicting winners.
- [ ] B. The p-value is too close to 0.05 to call this a significant difference. We do not observe a difference in performance.
- [X] C. The p-value is below 0.05, but the odds ratio is very close to 1. There is not a scientifically significant difference in performance.
- [ ] D. The p-value is below 0.05 and the odds ratio indicates that grade A- polls perform significantly better than grade C- polls.

## Comprehensive Assessment: Brexit

## Brexit poll analysis - Part 1

**Directions**

There are 12 multi-part problems in this comprehensive assessment that review concepts from the entire course. The problems are split over 3 pages. Make sure you read the instructions carefully and run all pre-exercise code.

For numeric entry problems, you have 10 attempts to input the correct answer. For true/false problems, you have 2 attempts.

If you have questions, visit the "Brexit poll analysis" discussion forum that follows the assessment.

IMPORTANT: Some of these exercises use dslabs datasets that were added in a July 2019 update. Make sure your package is up to date with the command update.packages("dslabs"). You can also update all packages on your system by running update.packages() with no arguments, and you should consider doing this routinely.

**Overview**

In June 2016, the United Kingdom (UK) held a referendum to determine whether the country would "Remain" in the European Union (EU) or "Leave" the EU. This referendum is commonly known as Brexit. Although the media and others interpreted poll results as forecasting "Remain" (p>0.5), the actual proportion that voted "Remain" was only 48.1% (p=0.481) and the UK thus voted to leave the EU. Pollsters in the UK were criticized for overestimating support for "Remain". 

In this project, you will analyze real Brexit polling data to develop polling models to forecast Brexit results. You will write your own code in R and enter the answers on the edX platform.

**Important definitions**

Data Import  
Import the brexit_polls polling data from the dslabs package and set options for the analysis:
```{r}
# suggested libraries and options
library(tidyverse)
options(digits = 3)
# load brexit_polls object
library(dslabs)
data(brexit_polls)
```

**Final Brexit parameters**

Define p=0.481 as the actual percent voting "Remain" on the Brexit referendum and  d=2p-1=-0.038 as the actual spread of the Brexit referendum with "Remain" defined as the positive outcome:
```{r}
p <- 0.481    # official proportion voting "Remain"
d <- 2*p-1    # official spread
```

#### Question 1: Expected value and standard error of a poll

```{r}
# The final proportion of voters choosing "Remain" was p=0.481. Consider a poll with a sample of N=1500 voters.
N <- 1500
# What is the expected total number of voters in the sample choosing "Remain"?
nb_remain <- N*p
nb_remain
# What is the standard error of the total number of voters in the sample choosing "Remain"?
se_remain <- sqrt(N*p*(1-p))
se_remain
# What is the expected value of X_bar, the proportion of "Remain" voters?
X_bar <- p
X_bar
# What is the standard error of X_bar, the proportion of "Remain" voters?
se <- sqrt(p*(1-p)/N)
se
# What is the expected value of d, the spread between the proportion of "Remain" voters and "Leave" voters?
d <- 2*p-1
d
# What is the standard error of d, the spread between the proportion of "Remain" voters and "Leave" voters?
2 * se
```

#### Question 2: Actual Brexit poll estimates
Load and inspect the brexit_polls dataset from dslabs, which contains actual polling data for the 6 months before the Brexit vote. Raw proportions of voters preferring "Remain", "Leave", and "Undecided" are available (remain, leave, undecided) The spread is also available (spread), which is the difference in the raw proportion of voters choosing "Remain" and the raw proportion choosing "Leave".

Calculate x_hat for each poll, the estimate of the proportion of voters choosing "Remain" on the referendum day (p=.481), given the observed spread and the relationship 𝑑̂=2𝑋̂−1. Use mutate to add a variable x_hat to the brexit_polls object by filling in the skeleton code below:

```{r}
head(brexit_polls)
brexit_polls <- brexit_polls %>%
        mutate(x_hat = (spread+1)/2)
# What is the average of the observed spreads (spread)?
mean(brexit_polls$spread)
 
# What is the standard deviation of the observed spreads?
sd(brexit_polls$spread)
# What is the average of x_hat, the estimates of the parameter p?
mean(brexit_polls$x_hat)
 
# What is the standard deviation of x_hat?
sd(brexit_polls$x_hat)
```

#### Question 3: Confidence interval of a Brexit poll
Consider the first poll in brexit_polls, a YouGov poll run on the same day as the Brexit referendum:

```{r}
brexit_polls[1,]
# Use qnorm to compute the 95% confidence interval for X_hat.
# What is the lower bound of the 95% confidence interval?
brexit_polls[1,]$x_hat - qnorm(.975) * sqrt(brexit_polls[1,]$x_hat *(1-brexit_polls[1,]$x_hat)/brexit_polls[1,]$samplesize)
 
# What is the upper bound of the 95% confidence interval?
brexit_polls[1,]$x_hat + qnorm(.975) * sqrt(brexit_polls[1,]$x_hat *(1-brexit_polls[1,]$x_hat)/brexit_polls[1,]$samplesize)
 
# Does the 95% confidence interval predict a winner (does not cover p=0.5)? Does the 95% confidence interval cover the true value of p observed during the referendum?
# A: The interval predicts a winner but does not cover the true value of p
```

## Brexit poll analysis - Part 2

This problem set is continued from the previous page. Make sure you have run the following code:
```{r}
# suggested libraries and options
library(tidyverse)
options(digits = 3)
# load brexit_polls object and add x_hat column
library(dslabs)
data(brexit_polls)
brexit_polls <- brexit_polls %>%
    mutate(x_hat = (spread + 1)/2)
# final proportion voting "Remain"
p <- 0.481
```

#### Question 4: Confidence intervals for polls in June
Create the data frame june_polls containing only Brexit polls ending in June 2016 (enddate of "2016-06-01" and later). We will calculate confidence intervals for all polls and determine how many cover the true value of d.

First, use mutate to calculate a plug-in estimate se_x_hat for the standard error of the estimate SE^[X] for each poll given its sample size and value of 𝑋̂ (x_hat). Second, use mutate to calculate an estimate for the standard error of the spread for each poll given the value of se_x_hat. 
Then, use mutate to calculate upper and lower bounds for 95% confidence intervals of the spread. Last, add a column hit that indicates whether the confidence interval for each poll covers the correct spread d=-0.038.

```{r}
d <- -0.038
june_polls <- brexit_polls %>%
  filter(enddate > "2016-06-01")
june_polls <- june_polls %>%
  mutate(se_x_hat = sqrt(x_hat*(1-x_hat)/samplesize),
           se_spread = 2 * se_x_hat,
           lower = spread - qnorm(.975) * se_spread,
           upper = spread + qnorm(.975) * se_spread,
           hit = (lower<=d & upper>=d))
head(june_polls)
# How many polls are in june_polls?
nrow(june_polls)
# What proportion of polls have a confidence interval that covers the value 0?
june_polls %>% summarize(mean(lower<=0 & upper>=0))
# What proportion of polls predict "Remain" (confidence interval entirely above 0)?
june_polls %>% summarize(mean(lower>0))
# What proportion of polls have a confidence interval covering the true value of d?
june_polls %>% summarize(mean(hit))
```

#### Question 5: Hit rate by pollster
Group and summarize the june_polls object by pollster to find the proportion of hits for each pollster and the number of polls per pollster. Use arrange to sort by hit rate.

```{r}
june_polls %>%
  group_by(pollster) %>%
  summarize(hit_rate=mean(hit), n()) %>%
  arrange(hit_rate)
```

Which of the following are TRUE?
A: The results are consistent with a large general bias that affects all pollsters. 

#### Question 6: Boxplot of Brexit polls by poll type
Make a boxplot of the spread in june_polls by poll type.

```{r}
june_polls %>% group_by(poll_type) %>%
  ggplot(aes(poll_type,spread)) +
  geom_boxplot()
```

Which of the following are TRUE?
A: Telephone polls tend to show support "Remain" (spread > 0).
A: Telephone polls tend to show higher support for "Remain" than online polls (higher spread).
A: Online polls have a larger interquartile range (IQR) for the spread than telephone polls, indicating that they are more variable.
A: Poll type introduces a bias that affects poll results.

#### Question 7: Combined spread across poll type
Calculate the confidence intervals of the spread combined across all polls in june_polls, grouping by poll type. Recall that to determine the standard error of the spread, you will need to double the standard error of the estimate.

Use this code (which determines the total sample size per poll type, gives each spread estimate a weight based on the poll's sample size, and adds an estimate of p from the combined spread) to begin your analysis:

```{r}
combined_by_type <- june_polls %>%
        group_by(poll_type) %>%
        summarize(N = sum(samplesize),
                  spread = sum(spread*samplesize)/N,
                  p_hat = (spread + 1)/2)
res <- combined_by_type %>%
  mutate(
    se_spread = 2 * sqrt(p_hat * (1-p_hat) / N),
    ci_lower = spread - qnorm(.975) * se_spread,
    ci_upper = spread + qnorm(.975) * se_spread
    )
res
#What is the lower bound of the 95% confidence interval for online voters?
#What is the upper bound of the 95% confidence interval for online voters?
res %>%
  filter(poll_type == 'Online') %>%
  select(ci_lower, ci_upper)
```

## Brexit poll analysis - Part 3

This problem set is continued from the previous page. Make sure you have run the following code:
```{r}
# suggested libraries and options
library(tidyverse)
options(digits = 3)
# load brexit_polls object and add x_hat column
library(dslabs)
data(brexit_polls)
brexit_polls <- brexit_polls %>%
    mutate(x_hat = (spread + 1)/2)
# final proportion voting "Remain"
p <- 0.481
```

#### Question 9: Chi-squared p-value
Define brexit_hit, with the following code, which computes the confidence intervals for all Brexit polls in 2016 and then calculates whether the confidence interval covers the actual value of the spread d=-0.038:
```{r}
d <- -0.038
brexit_hit <- brexit_polls %>%
  mutate(p_hat = (spread + 1)/2,
         se_spread = 2*sqrt(p_hat*(1-p_hat)/samplesize),
         spread_lower = spread - qnorm(.975)*se_spread,
         spread_upper = spread + qnorm(.975)*se_spread,
         hit = spread_lower < d & spread_upper > d) %>%
  select(poll_type, hit)
```

Use brexit_hit to make a two-by-two table of poll type and hit status. 
Then use the chisq.test function to perform a chi-squared test to determine whether the difference in hit rate is significant.

```{r}
head(brexit_hit)
res <- brexit_hit %>% 
  group_by(poll_type) %>%
  summarize(T=sum(hit), F=sum(!hit))
res
two_by_two <- tibble(hit=c(FALSE,TRUE), 
       'Online'=c(res[res$poll_type=='Online',]$F, 
                  res[res$poll_type=='Online',]$T),
       'Telephone'=c(res[res$poll_type!='Online',]$F, 
                  res[res$poll_type!='Online',]$T))
chisq_test <- two_by_two %>%
    select(-hit) %>%
  chisq.test()
chisq_test
# What is the p-value of the chi-squared test comparing the hit rate of online and telephone polls?
# A: 0.001
# Determine which poll type has a higher probability of producing a confidence interval that covers the correct value of the spread. Also determine whether this difference is statistically significant at a p-value cutoff of 0.05. Which of the following is true?
# A: Online polls are more likely to cover the correct value of the spread and this difference is statistically significant.
```

#### Question 10: Odds ratio of online and telephone poll hit rate
Use the two-by-two table constructed in the previous exercise to calculate the odds ratio between the hit rate of online and telephone polls to determine the magnitude of the difference in performance between the poll types.

```{r}
two_by_two
# Calculate the odds that an online poll generates a confidence interval that covers the actual value of the spread.
# Calculate the odds that a telephone poll generates a confidence interval that covers the actual value of the spread.
#Calculate the odds ratio to determine how many times larger the odds are for online polls to hit versus telephone polls.
odds_online <- two_by_two$Online[two_by_two$hit==TRUE] / two_by_two$Online[two_by_two$hit==FALSE]
odds_tel <- two_by_two$Telephone[two_by_two$hit==TRUE] / two_by_two$Telephone[two_by_two$hit==FALSE]
odds_online
odds_tel
odds_online / odds_tel
```

#### Question 11: Plotting spread over time
Use brexit_polls to make a plot of the spread (spread) over time (enddate) colored by poll type (poll_type). Use geom_smooth with method = "loess" to plot smooth curves with a span of 0.4. Include the individual data points colored by poll type. Add a horizontal line indicating the final value of d=-.038.

```{r}
brexit_polls %>%
  ggplot(aes(enddate, spread, col=poll_type)) +
  geom_point() +
  geom_smooth(method = "loess", span=0.4) +
  geom_hline(aes(yintercept=-.038))
```

#### Question 12: Plotting raw percentages over time
Use the following code to create the object brexit_long, which has a column vote containing the three possible votes on a Brexit poll ("remain", "leave", "undecided") and a column proportion containing the raw proportion choosing that vote option on the given poll:

```{r}
brexit_long <- brexit_polls %>%
    gather(vote, proportion, "remain":"undecided") %>%
    mutate(vote = factor(vote))
head(brexit_long)
# Make a graph of proportion over time colored by vote. Add a smooth trendline with geom_smooth and method = "loess" with a span of 0.3.
brexit_long %>%
  ggplot(aes(enddate, proportion, col=vote)) +
  geom_point() +
  geom_smooth(method = "loess", span=0.3)
```

Which of the following are TRUE?

* T: The percentage of undecided voters declines over time but is still around 10% throughout June.
* T: Over most of the date range, the confidence bands for "Leave" and "Remain" overlap.
* T: Over most of the date range, the confidence bands for "Leave" and "Remain" are below 50%.
* T: In the first half of June, "Leave" was polling higher than "Remain", although this difference was within the confidence intervals.
* F: At the time of the election in late June, the percentage voting "Leave" is trending upwards.
